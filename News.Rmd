---
title: "homework2"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
output:
  md_document:
    variant: markdown_github
---
By Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
library(foreach)
library(knitr)
library(kableExtra)
library(ggplot2)

dataset = read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv')

```
# Exercise 2.3
In this exercise, we analyzed data from 39,797 online articles published by Mashable during 2013 and 2014, in order to build a model to predict which article goes viral. The criteria used for defining an article as viral is if it surpasses the threshold of 1,400 shares. For each online article, we have 36 variables that can be included in the model (excluding URL since it is redundant for this analysis). 

We started by plotting a histogram to visualize the frequency distribution of the shares per article. 
```{r 1.2.1, echo=FALSE}
ggplot(data=dataset, aes(x = shares)) +
  geom_histogram(binwidth = 200, fill="lightgray", color="black", alpha=.6) +
  geom_vline(xintercept = 1400 , color="red" , linetype = "dashed") +
  labs(title = 'Shares Histogram', x = "# Shares", y = "Frequency")+
  xlim(0, 10000)+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), panel.grid.major.y = element_line(colour = "grey"))

```
If we observe the histogram of the number of shares, we can see that the data is skewed towards the lower end. It shows that although there are a broad range of articles that have well exceded 7,000 shares, a large portion of Mashable's articles are in the 800-1,200 shares range. A majority being slightly below our 1,400 shares threshold. 

As the variation of the shared articles is very broad, ranging from almost 0 shares to close to 10,000 shares, it made sense to log scale the dependant variable shares in order to compress down our data for uniformity.
```{r 1.2.2, echo=FALSE}
ggplot(data=dataset, aes(x = log(shares))) +
  geom_histogram(binwidth = 0.275, fill="lightgray", color="black", alpha=.6) +
  geom_vline(xintercept = log(1400) , color="red" , linetype = "dashed") +
  labs(title = 'Log Shares Histogram', x = "Log Shares", y = "Frequency")+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), panel.grid.major.y = element_line(colour = "grey"))
```
Since the data is more uniformly distributed in the log scale, we decided to use the transformed variable log(shares) in our predictive models.

We started the model selection by building a baseline model including all the available features as explanatory variables. So for our first linear model, we fitted all 36 variables in our model (except for URL) to make further assessments. 
```{r 1.2.3, echo=FALSE}
lm1 = lm(log(shares) ~ . - url, data=dataset)
```
From here, we removed the variables that yield an insignificant p-value such as the number of words in the content (n_tokens_content), maximum share of referenced articles (self_reference_max_share), maximum polarity of negative words (max_negative_polarity), etc. A larger (insignificant) p-value suggested that changes in the predictor were not associated with changes in the response. 
```{r}
lm2 = lm(shares ~ . - n_tokens_content - self_reference_max_shares - weekday_is_saturday
         - weekday_is_sunday - is_weekend - max_negative_polarity - min_negative_polarity, data=news_articles)
```
A step function was also created from this improved linear model to see how the variables interact with one another for feature selection of the variables. 
```{r 1.2.3, echo=FALSE}

```
In order to determine the most accurate model which accounts the relevant variables to see how articles reach the 1,400 shares threshold, we utilized a bootstrap to randomly split the original dataset in a training dataset containing 80% of the observations and a testing a dataset containing the remaining 20%. We then repeated this proccess 100 times. For each repetition, we fitted four linear models using the train dataset, and compared each model's error rate which signified what variables to consider to improve an article's chances of going viral. 

```{r 1.2.4, echo=FALSE}
err_vals = do(100)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_articles_train = news_articles[train_cases,]
  news_articles_test = news_articles[test_cases,]
  
lm1 = lm(shares ~ . , data = news_articles_test)
  
  lm2 =  lm(shares ~ . - n_tokens_content - self_reference_max_shares - weekday_is_saturday
            - weekday_is_sunday - is_weekend - max_negative_polarity - min_negative_polarity, data = news_articles_test)
  
  lm3 =  lm(shares ~ (. - n_tokens_content - self_reference_max_shares - weekday_is_saturday
            - weekday_is_sunday - is_weekend - max_negative_polarity - min_negative_polarity)^2, data = news_articles_test)
  
  lm4 = lm(formula = shares ~ n_tokens_title + num_hrefs + num_self_hrefs + 
             num_imgs + num_videos + average_token_length + num_keywords + 
             data_channel_is_lifestyle + data_channel_is_entertainment + 
             data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
             data_channel_is_world + self_reference_min_shares + self_reference_avg_sharess + 
             weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
             weekday_is_thursday + weekday_is_friday + global_rate_positive_words + 
             global_rate_negative_words + avg_positive_polarity + min_positive_polarity + 
             max_positive_polarity + avg_negative_polarity + title_subjectivity + 
             title_sentiment_polarity + abs_title_sentiment_polarity + 
             self_reference_min_shares:self_reference_avg_sharess, data=news_articles_test)
  yhat_test1 = predict(lm1, news_articles_test)
  yhat_test2 = predict(lm2, news_articles_test)
  yhat_test3 = predict(lm3, news_articles_test)
  yhat_test4 = predict(lm4, news_articles_test)
 
  c(error_rate(news_articles_test$shares, yhat_test1), error_rate(news_articles_test$shares, yhat_test2),
    error_rate(news_articles_test$shares, yhat_test3), error_rate(news_articles_test$shares, yhat_test4)) %>% round(3)
  
}
colMeans(err_vals)
```
We used the table shown below to assess the average error rate for each of the four models. All the results below were the averaged over 100 random train/test split samples in order to mollify the irregularities that could arise from randomization of samples. We calculated the average error rate of each model and from above we could we that our third linear model is our best model. The third model yields the smallest error rate of 0.04291. 

```{r 1.2.5, echo=FALSE}
#Error rate of all four models - colMeans(err_vals)
```
The table below reports the confusion matrix of the best model. 
```{r 1.2.5, echo=FALSE}
# confusion rate
lm_chosen = lm(shares ~ (. - n_tokens_content - self_reference_max_shares - weekday_is_saturday
                         - weekday_is_sunday - is_weekend - max_negative_polarity - min_negative_polarity)^2, data=news_articles)

news_articles$yhat = predict(lm_chosen, news_articles)

news_articles = mutate(news_articles, viral = ifelse(news_articles$share >= log(1400), 1, 0))
news_articles = mutate(news_articles, viral_hat = ifelse(news_articles$yhat >= log(1400), 1, 0))

xtabs(~viral + viral_hat, data=news_articles)
```
The reported confusion matrix gave an accuracy rate of 63.44%, a false negative rate of 2/(2+21152) = 0.0094%, false positive rate of 10/(21152+10) = 0.047%. Meanwhile the error rate was yielded to be ((2+21152)+(21152+10))/39,797 = 0.030%.
```{r 1.2.8, echo=FALSE}
confusion_matrix = table(y_test, yhat_test)
confusion_matrix
```
We compared this with a "null" model, which always predicted a 'viral' status and it gave an accuracy rate of 53.36%. Comparing this to our best model which gave an accuracy of 63.44%, the proposed model did much better. 

After this we approached this question from a classification perspective. The variable viral was defined as as having over 1,400 shares. Having the dependent variable as a binomial, we were able to utilize the logit models which are listed below.

As with linear models, we bootstrapped the models to randomly split the dataset into training and testing datasets containing 80% of observations and 20% of observations respectively. This process was repeated a 100 times. For each repetition, we fitted three logit models using the train dataset and compared each model's accuracy rate which signified what variables to consider to improve an article's chances of going viral. 

The first logit model that was used the base model we used before that accounted for all variables. Similarly, the second model is the step function that was created earlier to see how the variables interact with one another for feature selection. The third model that was chosen was the best linear model that gave the highest accuracy rate. 
```{r 1.2.9, echo=FALSE}

  glm_base = glm(viral ~  . , data = news_articles_test, family = 'binomial')
  
  #glm_step = glm(viral ~ . ,step(lm2, scope=~(.)^2, steps=1, data = news_articles_test, family = 'binomial'))
  
  glm_step = glm(formula = viral ~ lm_step, data = news_articles_test, family = 'binomial')
  
  glm_chosen = lm(shares ~ (. - n_tokens_content - self_reference_max_shares - weekday_is_saturday
                            - weekday_is_sunday - is_weekend - max_negative_polarity - min_negative_polarity)^2, 
                    data=news_articles, family = 'binomial')
  
```
As third logit model gave the lowest error rate, we would consider this to be the best logit model. The table below reports the confusion matrix of the best model. This yielded an accuracy rate of xx.xx%, the false negative rate of x/(x+x) = x.x% and the false positive rate of x/(x+x) = x%. Meanwhile the error rate is ((x+x)+(x+x))/x = x.x%.
```{r 1.2.10, echo=FALSE}
errMean = colMeans(errs_vals) %>% round(3)
err = matrix(errMean, nrow = 2, dimnames = 
               list(c("Classification Model", "Linear Model"), 
                    c("Overall Error", "True Positive", "False Positive"))) 

kable(err) %>% kable_styling("striped")
```
Having assessed the accuracy rates of the best models from the first and the second the approach, the logit/linear model gave a higher accuracy rate. 

A probable reason for this could be because of the nature of imperfectly relationship between the independent variables and log shares and so the logit model was the better fit. In addition, its also likely that the independant variables aren't good enough to predict the depenpendent variable. As a result classifying the dependant variable in a binary form would most certainly has reduced the spread of output (as the outputs were only viral or not) in turn made the logit model more accurate. 
```{r 1.2.11, echo=FALSE}
#KNN
```
In-sample accuracy

```{r 1.2.12, echo=FALSE}

```
KNN
```{r 1.2.13, echo=FALSE}

```
Again, we formalize training and testing sets. 
