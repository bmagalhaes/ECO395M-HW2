library(tidyverse)
library(mosaic)
library(FNN)
library(foreach)
data(SaratogaHouses)

#Baseline model= lm_medium with 11 main effects
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=SaratogaHouses)

# We are not considering making a new variable "Housevalue= price - landValue" although it seems reasonable.
# Once we change y variable, new models we build are not comparable with our original model, medium. 

# make a new variable age2=(age)^2 and add it into our original data, SaratogaHouses in order to make a new model with this variable later
SaratogaHouses <- mutate(SaratogaHouses, age2=(age)^2)

# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

#Hand-build some models and fit into training data

# model1: use all possible variables except rooms and age2 without interactions
# we get rid of "rooms" because of collinearity, rooms=bathrooms+bedrooms)
model1= lm(price ~ .-rooms-age2, data=saratoga_train)

# model2: simplify model1 by deleting some variables that look unimportant
# sewer(unimportant), waterfront, newConstruction(not enough samples with value of 1)
model2= lm(price~ .-rooms-sewer-waterfront-newConstruction-age2, data=saratoga_train)

# model3 : add all the interactions on model2
model3= lm(price~ (.-rooms-sewer-waterfront-newConstruction-age2)^2, data=saratoga_train)

# model4: allow only some interactions
model4= lm(price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# add a new variables for polynomial regression on model4
# we added age^2 because it is likely that house price rapidly goes down as it becomes aged

model5= lm(price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# our baseline model, medium
model_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# Predictions out of sample
yhat_test1 = predict(model1, saratoga_test)
yhat_test2 = predict(model2, saratoga_test)
yhat_test3 = predict(model3, saratoga_test)
yhat_test4 = predict(model4, saratoga_test)
yhat_test5 = predict(model5, saratoga_test)
yhat_test_medium = predict(model_medium, saratoga_test)

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}

# Root mean-squared prediction error
rmse(saratoga_test$price, yhat_test1)
rmse(saratoga_test$price, yhat_test2)
rmse(saratoga_test$price, yhat_test3)
rmse(saratoga_test$price, yhat_test4)
rmse(saratoga_test$price, yhat_test5)
rmse(saratoga_test$price, yhat_test_medium)

# now, do Monte Carlo simulation 1000 times with different training-test sets and find each mean of RSME

rmse_check = do(100)*{# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

model1= lm(price ~ .-rooms-age2, data=saratoga_train)
model2= lm(price~ .-rooms-sewer-waterfront-newConstruction-age2, data=saratoga_train)
model3= lm(price~ (.-rooms-sewer-waterfront-newConstruction-age2)^2, data=saratoga_train)
model4= lm(price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)
model5= lm(price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)
model_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# Predictions out of sample
yhat_test1 = predict(model1, saratoga_test)
yhat_test2 = predict(model2, saratoga_test)
yhat_test3 = predict(model3, saratoga_test)
yhat_test4 = predict(model4, saratoga_test)
yhat_test5 = predict(model5, saratoga_test)
yhat_test_medium = predict(model_medium, saratoga_test)

c(rmse(saratoga_test$price, yhat_test1), rmse(saratoga_test$price, yhat_test2), rmse(saratoga_test$price, yhat_test3), rmse(saratoga_test$price, yhat_test4), rmse(saratoga_test$price, yhat_test5), rmse(saratoga_test$price, yhat_test_medium))}

rmse_result= data.frame('model1'= colMeans(rmse_check[1]), 'model2'=colMeans(rmse_check[2]), 'model3'=colMeans(rmse_check[3]), 'model4'=colMeans(rmse_check[4]), 'model5'=colMeans(rmse_check[5]), 'model_medium'=colMeans(rmse_check[6]))

rmse_result

# We can see that model 1 shows the least rmse

# Which variable is the strongest driver of house prices?
  test each of them by deleting a variable and check rmse again
  
  
rmse_check2 = do(100)*{# Split into training and testing sets
train_cases2 = sample.int(n, n_train, replace=FALSE)
test_cases2 = setdiff(1:n, train_cases)
saratoga_train2 = SaratogaHouses[train_cases2,]
saratoga_test2 = SaratogaHouses[test_cases2,]

#Hand-build some models and fit into training data

# model1_wo_lotSize: without lotSize
model1_wo_lotSize= lm(price ~ .-rooms-age2-lotSize, data=saratoga_train2)

# model1_wo_age: without age
model1_wo_age= lm(price ~ .-rooms-age2-age, data=saratoga_train2)

# model1_wo_landValue: without landValue
model1_wo_landValue= lm(price ~ .-rooms-age2-landValue, data=saratoga_train2)

# model1_wo_livingArea: without livingArea
model1_wo_livingArea= lm(price ~ .-rooms-age2-livingArea, data=saratoga_train2)

# model1_wo_pctCollege: without pctCollege
model1_wo_pctCollege= lm(price ~ .-rooms-age2-pctCollege, data=saratoga_train2)

# model1_wo_bedrooms: without bedrooms
model1_wo_bedrooms= lm(price ~ .-rooms-age2-bedrooms, data=saratoga_train2)

# model1_wo_fireplaces: without fireplaces
model1_wo_fireplaces= lm(price ~ .-rooms-age2-fireplaces, data=saratoga_train2)

# model1_wo_bathrooms: without bathrooms
model1_wo_bathrooms= lm(price ~ .-rooms-age2-bathrooms, data=saratoga_train2)

# model1_wo_heating: without heating
model1_wo_heating= lm(price ~ .-rooms-age2-heating, data=saratoga_train2)

# model1_wo_fuel: without fuel
model1_wo_fuel= lm(price ~ .-rooms-age2-fuel, data=saratoga_train2)

# model1_wo_sewer: without sewer
model1_wo_sewer= lm(price ~ .-rooms-age2-sewer, data=saratoga_train2)

# model1_wo_waterfront: without waterfront
model1_wo_waterfront= lm(price ~ .-rooms-age2-waterfront, data=saratoga_train2)

# model1_wo_newConstruction: without newConstruction
model1_wo_newConstruction= lm(price ~ .-rooms-age2-newConstruction, data=saratoga_train2)

# model1_wo_centralAir: without centralAir
model1_wo_centralAir= lm(price ~ .-rooms-age2-centralAir, data=saratoga_train2)

# Predictions out of sample
yhat_test_lotSize = predict(model1_wo_lotSize, saratoga_test2)
yhat_test_age = predict(model1_wo_age, saratoga_test2)
yhat_test_landValue = predict(model1_wo_landValue, saratoga_test2)
yhat_test_livingArea = predict(model1_wo_livingArea, saratoga_test2)
yhat_test_pctCollege = predict(model1_wo_pctCollege, saratoga_test2)
yhat_test_bedrooms = predict(model1_wo_bedrooms, saratoga_test2)
yhat_test_fireplaces = predict(model1_wo_fireplaces, saratoga_test2)
yhat_test_bathrooms = predict(model1_wo_bathrooms, saratoga_test2)
yhat_test_heating = predict(model1_wo_heating, saratoga_test2)
yhat_test_fuel = predict(model1_wo_fuel, saratoga_test2)
yhat_test_sewer = predict(model1_wo_sewer, saratoga_test2)
yhat_test_waterfront = predict(model1_wo_waterfront, saratoga_test2)
yhat_test_newConstruction = predict(model1_wo_newConstruction, saratoga_test2)
yhat_test_centralAir = predict(model1_wo_centralAir, saratoga_test2)

c(rmse(saratoga_test2$price, yhat_test_lotSize), rmse(saratoga_test2$price, yhat_test_age), rmse(saratoga_test2$price, yhat_test_landValue), rmse(saratoga_test2$price, yhat_test_livingArea), rmse(saratoga_test2$price, yhat_test_pctCollege), rmse(saratoga_test2$price, yhat_test_bedrooms), rmse(saratoga_test2$price, yhat_test_fireplaces), rmse(saratoga_test2$price, yhat_test_bathrooms), rmse(saratoga_test2$price, yhat_test_heating), rmse(saratoga_test2$price, yhat_test_fuel), rmse(saratoga_test2$price, yhat_test_sewer), rmse(saratoga_test2$price, yhat_test_waterfront), rmse(saratoga_test2$price, yhat_test_newConstruction), rmse(saratoga_test2$price, yhat_test_centralAir))}

rmse_result2= data.frame('model_wo_lotSize'= colMeans(rmse_check2[1]), 'model_wo_age'=colMeans(rmse_check2[2]), 'model_wo_landValue'=colMeans(rmse_check2[3]), 'model_wo_livingArea'=colMeans(rmse_check2[4]), 'model_wo_pctCollege'=colMeans(rmse_check2[5]), 'model_wo_bedrooms'=colMeans(rmse_check2[6]), 'model_wo_fireplaces'= colMeans(rmse_check2[7]), 'model_wo_bathrooms'=colMeans(rmse_check2[8]), 'model_wo_heating'=colMeans(rmse_check2[9]), 'model_wo_fuel'=colMeans(rmse_check2[10]), 'model_wo_sewer'=colMeans(rmse_check2[11]), 'model_wo_waterfront'=colMeans(rmse_check2[12]), 'model_wo_newConstruction'=colMeans(rmse_check2[13]), 'model_wo_centralAir'=colMeans(rmse_check2[14]))

rmse_result2

# Rmse gets the biggest when "landValue" variable isn't there.
# Therefore we can say "landValue" is the strongest driver of house prices.

# KNN modeling

# construct the training and test-set feature matrices
Xtrain= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train)
Xtest = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test)

# training and testing set responses 
ytrain = saratoga_train$price
ytest = saratoga_test$price

# rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale= scale_train)
K= 10

# fit the model
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)

# calculate test-set performance
rmse(ytest, knn_model$pred)
rmse(ytest, yhat_test1)

k_grid = seq(1,50, by=1)
rmse_grid = foreach(K= k_grid, .combine='c') %do% {
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)
rmse(ytest, knn_model$pred)
}

plot(k_grid, rmse_grid, ylim=c(55000,80000))
abline(h=rmse(ytest, yhat_test1), col='red')
# We can see our linear model is better than knn_model whatever value k has

K= which.min(rmse_grid)
rmse(ytest, knn_model$pred)
rmse(ytest, yhat_test1)

# the minimized rmse of knn-reg is still bigger than the rmse of our best-fit linear model

# now let's do knn-reg 100 times and find out the mean of each minimum rmse value

rmse_check3 = do(100)*{# Split into training and testing sets
train_cases3 = sample.int(n, n_train, replace=FALSE)
test_cases3 = setdiff(1:n, train_cases3)
saratoga_train3 = SaratogaHouses[train_cases3,]
saratoga_test3 = SaratogaHouses[test_cases3,]

# construct the training and test-set feature matrices
Xtrain3= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train3)
Xtest3 = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test3)

# training and testing set responses 
ytrain3 = saratoga_train3$price
ytest3 = saratoga_test3$price

# rescale
scale_train3 = apply(Xtrain3, 2, sd)
Xtilde_train3 = scale(Xtrain3, scale = scale_train3)
Xtilde_test3 = scale(Xtest3, scale= scale_train3)
K3=10

# fit the model
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)


k_grid3 = seq(1,50, by=1)
rmse_grid3 = foreach(K3= k_grid3, .combine='c') %do% {
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)
rmse(ytest3, knn_model3$pred)}
min(rmse_grid3)
}


# When we compare the means, we can see our linear regression predicts better values than the knn regression

colMeans(rmse_check3[1])
rmse_result[1]



