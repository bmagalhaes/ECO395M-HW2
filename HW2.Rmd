---
title: "homework1"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
always_allow_html: true
output:
    md_document:
    variant: markdown_github
---
#ECO 395M: Exercise 2

Bernardo Arreal Magalhaes - UTEID ba25727

Adhish Luitel - UTEID al49674

Ji Heon Shim - UTEID js93996

## Exercise 2.1
```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(margins)
library(dplyr)
library(kableExtra)
library(sjPlot)
library(mosaic)
library(FNN)
library(foreach)
library(jtools)
data(SaratogaHouses)
```

In this exercise, we hand-build five models for price in order to find out the best one which outperforms the "medium" model that we considered in class.

```{r 2.1.1, echo= TRUE, warning = FALSE}
model1= price ~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + sewer + waterfront + newConstruction + centralAir 
model2= price~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + centralAir 
model3= price~ (lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + centralAir)^2 
model4= price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir
model5= price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir

model_medium = price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir
```

Here are the main features of our models.

Model 1 : include all main effects except roooms
(exclude rooms because of colinearity, rooms = bedrooms + batherooms)
Model 2: simplify model1 by reducing some variables(-sewer-waterfront-newConstruction)
Model 3: add all the interactions on model 2
Model 4: allow only some interactions on model 2
Model 5: a polynomial model by adding age^2 on model 4
Model_medium: baseline model with 11 main effects
             
```{r 2.1.2, echo=FALSE}
# make a new variable age2=(age)^2
SaratogaHouses <- mutate(SaratogaHouses, age2=(age)^2)

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
```

In order to measure performances of each model, we run Monte Carlo training-test split(train 80%, test 20%) for 100 times and calcaulate the average values of out-of-sample RMSE for each model.

```{r 2.1.3, echo=FALSE, warning=FALSE}
rmse_check = do(100)*{# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

#Hand-build some models and fit into training data

# model1: use all possible variables except rooms and age2 without interactions
# we get rid of "rooms" because of collinearity, rooms=bathrooms+bedrooms)
model1= lm(price ~ .-rooms-age2, data=saratoga_train)

# model2: simplify model1 by deleting some variables that look unimportant
# sewer(unimportant), waterfront, newConstruction(not enough samples with value of 1)
model2= lm(price~ .-rooms-sewer-waterfront-newConstruction-age2, data=saratoga_train)

# model3 : add all the interactions on model2
model3= lm(price~ (.-rooms-sewer-waterfront-newConstruction-age2)^2, data=saratoga_train)

# model4: allow only some interactions
model4= lm(price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# add a new variables for polynomial regression on model4
# we added age^2 because it is likely that house price rapidly goes down as it becomes aged

model5= lm(price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# our baseline model, medium
model_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# Predictions out of sample
yhat_test1 = predict(model1, saratoga_test)
yhat_test2 = predict(model2, saratoga_test)
yhat_test3 = predict(model3, saratoga_test)
yhat_test4 = predict(model4, saratoga_test)
yhat_test5 = predict(model5, saratoga_test)
yhat_test_medium = predict(model_medium, saratoga_test)

c(rmse(saratoga_test$price, yhat_test1), rmse(saratoga_test$price, yhat_test2), rmse(saratoga_test$price, yhat_test3), rmse(saratoga_test$price, yhat_test4), rmse(saratoga_test$price, yhat_test5), rmse(saratoga_test$price, yhat_test_medium))}

rmse_result= data.frame("model1"= colMeans(rmse_check[1]), "model2"=colMeans(rmse_check[2]), "model3"=colMeans(rmse_check[3]), "model4"=colMeans(rmse_check[4]), "model5"=colMeans(rmse_check[5]), "model medium"=colMeans(rmse_check[6]), row.names="AVG RMSE")

kable(rmse_result) %>% kable_styling("striped")

```
  
The best model turned out to be model 1 with the least out-of-sample RMSE value. Here is the summary of model 1.

```{r 2.1.4, echo=TRUE}
summ(model1)
```

And we can find the variable which is the strongest driver of house prices by assessing how much it improves the out-of-sample RMSE when it is included in the model.
So we test how much the out-of-sample increases when we exclude a certain variable from our model 1, and get the average RMSE by doing Monte Carlo simulation on different training-test sets(80%-20%) for 100 times.

As the table below shows, "landValue" variable seems to be the storngest drive of house prices. This result might be caused because land values are already included in house prices(House price = Land value + Pure house value), so they are strongly related to each other.

```{r 2.1.5, echo=FALSE, warning=FALSE}
rmse_check2 = do(100)*{# Split into training and testing sets
train_cases2 = sample.int(n, n_train, replace=FALSE)
test_cases2 = setdiff(1:n, train_cases)
saratoga_train2 = SaratogaHouses[train_cases2,]
saratoga_test2 = SaratogaHouses[test_cases2,]

#Hand-build some models and fit into training data

# model1_wo_lotSize: without lotSize
model1_wo_lotSize= lm(price ~ .-rooms-age2-lotSize, data=saratoga_train2)

# model1_wo_age: without age
model1_wo_age= lm(price ~ .-rooms-age2-age, data=saratoga_train2)

# model1_wo_landValue: without landValue
model1_wo_landValue= lm(price ~ .-rooms-age2-landValue, data=saratoga_train2)

# model1_wo_livingArea: without livingArea
model1_wo_livingArea= lm(price ~ .-rooms-age2-livingArea, data=saratoga_train2)

# model1_wo_pctCollege: without pctCollege
model1_wo_pctCollege= lm(price ~ .-rooms-age2-pctCollege, data=saratoga_train2)

# model1_wo_bedrooms: without bedrooms
model1_wo_bedrooms= lm(price ~ .-rooms-age2-bedrooms, data=saratoga_train2)

# model1_wo_fireplaces: without fireplaces
model1_wo_fireplaces= lm(price ~ .-rooms-age2-fireplaces, data=saratoga_train2)

# model1_wo_bathrooms: without bathrooms
model1_wo_bathrooms= lm(price ~ .-rooms-age2-bathrooms, data=saratoga_train2)

# model1_wo_heating: without heating
model1_wo_heating= lm(price ~ .-rooms-age2-heating, data=saratoga_train2)

# model1_wo_fuel: without fuel
model1_wo_fuel= lm(price ~ .-rooms-age2-fuel, data=saratoga_train2)

# model1_wo_sewer: without sewer
model1_wo_sewer= lm(price ~ .-rooms-age2-sewer, data=saratoga_train2)

# model1_wo_waterfront: without waterfront
model1_wo_waterfront= lm(price ~ .-rooms-age2-waterfront, data=saratoga_train2)

# model1_wo_newConstruction: without newConstruction
model1_wo_newConstruction= lm(price ~ .-rooms-age2-newConstruction, data=saratoga_train2)

# model1_wo_centralAir: without centralAir
model1_wo_centralAir= lm(price ~ .-rooms-age2-centralAir, data=saratoga_train2)

# Predictions out of sample
yhat_test_lotSize = predict(model1_wo_lotSize, saratoga_test2)
yhat_test_age = predict(model1_wo_age, saratoga_test2)
yhat_test_landValue = predict(model1_wo_landValue, saratoga_test2)
yhat_test_livingArea = predict(model1_wo_livingArea, saratoga_test2)
yhat_test_pctCollege = predict(model1_wo_pctCollege, saratoga_test2)
yhat_test_bedrooms = predict(model1_wo_bedrooms, saratoga_test2)
yhat_test_fireplaces = predict(model1_wo_fireplaces, saratoga_test2)
yhat_test_bathrooms = predict(model1_wo_bathrooms, saratoga_test2)
yhat_test_heating = predict(model1_wo_heating, saratoga_test2)
yhat_test_fuel = predict(model1_wo_fuel, saratoga_test2)
yhat_test_sewer = predict(model1_wo_sewer, saratoga_test2)
yhat_test_waterfront = predict(model1_wo_waterfront, saratoga_test2)
yhat_test_newConstruction = predict(model1_wo_newConstruction, saratoga_test2)
yhat_test_centralAir = predict(model1_wo_centralAir, saratoga_test2)

c(rmse(saratoga_test2$price, yhat_test_lotSize), rmse(saratoga_test2$price, yhat_test_age), rmse(saratoga_test2$price, yhat_test_landValue), rmse(saratoga_test2$price, yhat_test_livingArea), rmse(saratoga_test2$price, yhat_test_pctCollege), rmse(saratoga_test2$price, yhat_test_bedrooms), rmse(saratoga_test2$price, yhat_test_fireplaces), rmse(saratoga_test2$price, yhat_test_bathrooms), rmse(saratoga_test2$price, yhat_test_heating), rmse(saratoga_test2$price, yhat_test_fuel), rmse(saratoga_test2$price, yhat_test_sewer), rmse(saratoga_test2$price, yhat_test_waterfront), rmse(saratoga_test2$price, yhat_test_newConstruction), rmse(saratoga_test2$price, yhat_test_centralAir))}

rmse_result2= data.frame("model wo lotSize"= colMeans(rmse_check2[1]), "model wo age"=colMeans(rmse_check2[2]), "model wo landValue"=colMeans(rmse_check2[3]), "model wo livingArea"=colMeans(rmse_check2[4]), "model wo pctCollege"=colMeans(rmse_check2[5]), "model wo bedrooms"=colMeans(rmse_check2[6]), "model wo fireplaces"= colMeans(rmse_check2[7]), "model wo bathrooms"=colMeans(rmse_check2[8]), "model wo heating"=colMeans(rmse_check2[9]), "model wo fuel"=colMeans(rmse_check2[10]), "model wo sewer"=colMeans(rmse_check2[11]), "model wo waterfront"=colMeans(rmse_check2[12]), "model wo newConstruction"=colMeans(rmse_check2[13]), "model wo centralAir"=colMeans(rmse_check2[14]), row.names="AVG RMSE")

rmse_result2_t = t(rmse_result2)

kable(rmse_result2_t) %>% kable_styling("striped")
```

Now, we build a nonparametic KNN model to compare it with our linear model and figure out which one performs better.
By using the same train and test sets that we used in our linear regression, the result shows that whatever value K may have, the knn model is unlikely to perfrom better than our linear model.
In the graph below, the horizontal red line shows the out-of-sample RMSE of our linear model. We can see that all the RMSEs of the knn model in accordance with k values are plotted above the red line. 
And the table below suggests the fact that the minimum RMSE value of knn model is still bigger than our best-fit linear model. 

```{r 2.1.6, echo=FALSE}
# construct the training and test-set feature matrices
Xtrain= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train)
Xtest = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test)

# training and testing set responses 
ytrain = saratoga_train$price
ytest = saratoga_test$price

# rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale= scale_train)
K= 10

# fit the model
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)

k_grid = seq(1,50, by=1)
rmse_grid = foreach(K= k_grid, .combine='c') %do% {
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)
rmse(ytest, knn_model$pred)
}

ggplot() + 
  geom_point(mapping = aes(x = k_grid, y = rmse_grid), color="black") + 
  geom_hline(yintercept = rmse(ytest, yhat_test1), color = "red", size=1) +
  labs(x = "K", y = "RMSE") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank()) +
  scale_y_continuous(breaks=c(65000, 70000, 75000, 80000, 85000))

K= which.min(rmse_grid)
modelcompare = data.frame('Kmin'=K, "knn model RMSE" = rmse(ytest, knn_model$pred), "linear model RMSE" = rmse(ytest, yhat_test1))
kable(modelcompare) %>% kable_styling("striped", full_width=F)
```

But there is random variation due to the particular choice of data points that end up in your train/test split. So we run Monte-Carlo simulation again using random train/test split for 100 times, and compare the minimim RMSE of knn model with RMSE of our linear model.
As a result, we can see that our linear model outperforms the knn model.

```{r 2.1.7, echo=FALSE}
rmse_check3 = do(100)*{# Split into training and testing sets
train_cases3 = sample.int(n, n_train, replace=FALSE)
test_cases3 = setdiff(1:n, train_cases3)
saratoga_train3 = SaratogaHouses[train_cases3,]
saratoga_test3 = SaratogaHouses[test_cases3,]

# construct the training and test-set feature matrices
Xtrain3= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train3)
Xtest3 = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test3)

# training and testing set responses 
ytrain3 = saratoga_train3$price
ytest3 = saratoga_test3$price

# rescale
scale_train3 = apply(Xtrain3, 2, sd)
Xtilde_train3 = scale(Xtrain3, scale = scale_train3)
Xtilde_test3 = scale(Xtest3, scale= scale_train3)
K3=10

# fit the model
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)


k_grid3 = seq(1,50, by=1)
rmse_grid3 = foreach(K3= k_grid3, .combine='c') %do% {
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)
rmse(ytest3, knn_model3$pred)}
min(rmse_grid3)
}


modelcompare2 = data.frame("knn model"= colMeans(rmse_check3), "linear model"= colMeans(rmse_check[1]), row.names="Average RMSE")
kable(modelcompare2) %>% kable_styling("striped")

```

## Exercise 2.2

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(margins)
library(dplyr)
library(kableExtra)
library(sjPlot)
library(mosaic)

brca = read.csv(url("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW2/master/brca.csv"))
```

This exercise is based on a dataset consisting of 987 screening mammograms administered at a hospital in Seattle, Washington. The goal of the analysis is to evaluate the performance of five different radiologists considering several risk factors.

First, we analyzed the raw data to verify whether each radiologist has a different recall rate (A.recallrate) or not, and compare precision (B.cancerrate) and false negative (C.false_negative) rates. We can observe that, even though radiologist89 has a higher probability of recalling patients, his false negative error rate (not recalling patients that actually have cancer) doesn't substantially differ from radiologist95 and radiologist34, who have the lowest recall rates.

```{r 2.2.1, echo=FALSE}
recall_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(A.recallrate = length(which(recall == 1)) / (length(which(recall == 1))
            + length(which(recall == 0))))

recall_rate = recall_rate[order(-recall_rate$A.recallrate),c(1,2)]
recall_rate = mutate(recall_rate, n = row_number())

cancer_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(B.cancerrate = length(which(recall == 1 & cancer == 1)) / (length(which(recall == 1 & 
            cancer == 1)) + length(which(recall == 1 & cancer == 0))))

error_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(C.false_negative = length(which(recall == 0 & cancer == 1)) / (length(which(recall == 0 & 
            cancer == 1)) + length(which(recall == 0 & cancer == 0))))

radiologist = left_join(recall_rate, cancer_rate, by = "radiologist")
radiologist = left_join(radiologist, error_rate, by = "radiologist")

radiologist_long = radiologist %>%
  gather("Stat", "Value", -radiologist, -n)
radiologist_long = mutate(radiologist_long, Value = Value*100)

radiologist_89 = radiologist_long[which(radiologist_long$n==1), ]

ggplot(data = radiologist_long) + 
  geom_bar(mapping = aes(x=reorder(radiologist, -n), y=Value),
           stat='identity', position ='dodge', fill="lightgray", color="black", alpha=.6, width=.5) + 
  facet_wrap(~Stat) + 
  coord_flip() +
  labs(y="Observed rate (in percentage)", x = "") +
  geom_hline(data= radiologist_89, aes(yintercept=Value), color="red" , linetype = "dashed") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) 
```

However, since each radiologist read the mammograms of a different set of patients, this difference could be explained by the fact that some radiologists might have seen patients whose clinical situation required them to be recalled for further examination.

In order to analyse if, holding patient risk factors equal, some radiologists are more clinically conservative than others in recalling patients, we built two classification models:

```{r 2.2.2, echo=TRUE}
model1 = recall ~ radiologist + age + history + symptoms + menopause + density
model2 = recall ~ (age + history + symptoms + menopause + density) * radiologist
```

The table below shows the Average Marginal Effect (AME) for each radiologist in the two models. We can see that, when using Model 1, radiologist89 is the most conservative - holding patient risk factors equal, the probability of being recalled increases by 5.71 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

When allowing for interactions between radiologist and each control variable as in Model 2, radiologist89 is still the most conservative - holding patient risk factors equal, the probability of being recalled increases by 17.48 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

```{r 2.2.3, echo=FALSE, warning = FALSE}
brca2 = brca
brca2$radiologist = str_replace(brca2$radiologist, "radiologist","")
brca2$age = str_replace(brca2$age, "age","")
brca2$menopause = str_replace(brca2$menopause, "meno","")
brca2$density = str_replace(brca2$density, "density","")

brca_new = dummy.data.frame(brca2, names = c("radiologist", "age","menopause", "density") , sep = ".")
brca_new = subset(brca_new, select = -c(radiologist.13, age.4049,menopause.postHT, density.1))

model_1 = glm(recall ~ . - cancer, data=brca_new, family=binomial)
ame_11 = summary(margins(model_1, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

model_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
              + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
              + radiologist.66 + radiologist.89 + radiologist.95), data=brca_new, family=binomial)
ame_21 = summary(margins(model_2, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

ame_11 = select(ame_11,-c(4:7))
ame_21 = select(ame_21,-c(4:7))

ame = left_join(ame_11, ame_21, by = "factor")
colnames(ame) <- c("","AME","se","AME","se")

kable(ame) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "Model 1" = 2, "Model 2" = 2))
```

Finally, in order to estimate how each radiologist would perform when facing the same set of patients, we used a bootstrap to randomly split the original dataset in a training dataset containing 80% of the observations and a testing a dataset containing 20% of the observations, repeating the proccess 100 times. In each repetition, we fitted both models using the train dataset, and compared each model's predictions when all radiologists, in a hypothetical scenario, analyze the entire test dataset. 

We computed average probability of recall per repetition, and calculated the average of the 100 samples to mitigate the effect of randomization. For both models, radiologist89 has the highest probability of recall, followed by radiologist66, radiologist13, radiologist95 and radiologist 34.

```{r 2.2.4, echo=FALSE, warning = FALSE}
n = nrow(brca_new)

boot1 = do(100)*{
  
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  brca_train = brca_new[train_cases,]
  brca_test = brca_new[test_cases,]
  brca_test = brca_test[rep(seq_len(nrow(brca_test)), 5),]
  brca_test =  brca_test %>%
    mutate(n = row_number()) %>%
    mutate(radiologist.34 = 0) %>%
    mutate(radiologist.66 = 0) %>%
    mutate(radiologist.89 = 0) %>%
    mutate(radiologist.95 = 0)
  brca_test = brca_test %>%
    mutate(radiologist.34 = ifelse(n > nrow(brca_test)/5 & nrow(brca_test)*2/5 >= n, 1, 0)) %>%
    mutate(radiologist.66 = ifelse(n > nrow(brca_test)*2/5 & nrow(brca_test)*3/5 >= n, 1, 0)) %>%
    mutate(radiologist.89 = ifelse(n > nrow(brca_test)*3/5 & nrow(brca_test)*4/5 >= n, 1, 0)) %>%
    mutate(radiologist.95 = ifelse(n > nrow(brca_test)*4/5 & nrow(brca_test) >= n, 1, 0))
 
  # fit to this training set
  glm_1 = glm(recall ~ . - cancer, data=brca_train, family=binomial)
  glm_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
                        + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
                        + radiologist.66 + radiologist.89 + radiologist.95), data=brca_train, family=binomial)
  
  # predict on this testing set
  yhat_test1 = predict(glm_1, brca_test, type="response")
  yhat_test2 = predict(glm_2, brca_test, type="response")
  brca_test = brca_test %>%
    mutate(radiologist = ifelse(radiologist.34 == 1, "radiologist.34",
                         ifelse(radiologist.66 == 1, "radiologist.66",
                         ifelse(radiologist.89 == 1, "radiologist.89",
                         ifelse(radiologist.95 == 1, "radiologist.95", "radiologist.13")))))
  
  brca_test$pred1 = yhat_test1
  brca_test$pred2 = yhat_test2
  recalls_1 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred1))
  recalls_2 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred2))
  recalls_t = rbind(recalls_1, recalls_2)
  recalls = as.data.frame(t(recalls_t))
  recalls = recalls[-1, ]
  colnames(recalls) <- c("M1_radiologist.13", "M1_radiologist.34", "M1_radiologist.66", 
                         "M1_radiologist.89", "M1_radiologist.95", "M2_radiologist.13",
                         "M2_radiologist.34", "M2_radiologist.66", 
                         "M2_radiologist.89", "M2_radiologist.95")
  recalls
  }

boot1[, c(1:10)] = sapply(boot1[, c(1:10)], as.numeric)

radiologist = c("radiologist.13", "radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95")
Model_1 = c(mean(boot1$M1_radiologist.13), mean(boot1$M1_radiologist.34), mean(boot1$M1_radiologist.66),
       mean(boot1$M1_radiologist.89), mean(boot1$M1_radiologist.95))
Model_2 = c(mean(boot1$M2_radiologist.13), mean(boot1$M2_radiologist.34), mean(boot1$M2_radiologist.66),
       mean(boot1$M2_radiologist.89), mean(boot1$M2_radiologist.95))
a = data.frame(radiologist, Model_1,Model_2)

kable(a) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "(Out of sample) Average recall probability" = 2))
```

The results corroborate what we observed in the AME table, that some radiologists are more clinically conservative than others in recalling patients, even holding patient risk factors equal.

After that, we analyzed if the data suggests that radiologists at this hospital should be weighting some clinical risk factors more heavily than they currently are when interpreting mammograms in order to make a decision on whether to recall a patient or not.

We started by stablishing the following baseline model.

```{r 2.2.5, echo=TRUE}
baseline_model = cancer ~ recall
```

Then, we built multiple models adding each of the clinical risk factors to the baseline model to evaluate the results separately.

```{r 2.2.6, echo=TRUE}
baseline_history_model = cancer ~ recall + history
baseline_age_model = cancer ~ recall + age
baseline_symptoms_model = cancer ~ recall + symptoms
baseline_menopause_model = cancer ~ recall + menopause
baseline_density_model = cancer ~ recall + density
```

If the radiologists were appropriately accounting for the clinical risk factors when deciding whether to recall a patient for further examination, we would expect the coefficient associated with the recall variable to capture this effect, and the coefficient of the control variable to be close to zero (so the odds ratio to be close to one). Hence, including this variables in the model shouldn't considerably affect the cancer predictions.

The table below summarizes the estimates from the 6 models. Here, we observe that age 70 plus, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status are some factors that are increasing the odds of having a cancer, even after medical analysis! Thus there's extra information in the risk factors that the doctors should be weighting more heavily to recall patients than they currently are.

```{r 2.2.7, echo=FALSE, warning = FALSE}
model_cancer = glm(cancer ~ recall, data=brca, family=binomial)
model_cancer2 = glm(cancer ~ recall + history, data=brca, family=binomial)
model_cancer3 = glm(cancer ~ recall + age, data=brca, family=binomial)
model_cancer4 = glm(cancer ~ recall + symptoms, data=brca, family=binomial)
model_cancer5 = glm(cancer ~ recall + menopause, data=brca, family=binomial)
model_cancer6 = glm(cancer ~ recall + density, data=brca, family=binomial)

tab_model(model_cancer, model_cancer2, model_cancer3, model_cancer4, model_cancer5,
          model_cancer6, show.ci = FALSE, show.p = TRUE,
          dv.labels = c("Baseline Model", "+History", "+Age", "+Symptoms",
                        "+Menopause", "+Density"))
```

Considering the results above, we built a new model including those 3 risk factors.

```{r 2.2.8, echo=TRUE}
proposed_model = cancer ~ recall + age.70plus + postmenounknown + density.4
```

In order to assess how well this model performs when predicting cancer status, we need to compare its predictions with the predictions from the baseline model.

The confusion matrix for the baseline model below shows that the doctors are currently predicting cancer with a 59.46% sensitivity (22/(22+15)) and a accuracy rate of 85.71% ((824+22)/987).

```{r 2.2.9, echo=FALSE, warning = FALSE}
xtabs(~cancer + recall, data=brca)
```

Considering the in-sample probability of cancer as a threshold for the fitted values of the proposed model to predict cancer or not, we computed the following confusion matrix. This matrix shows that, when including age.70plus, postmenounknown and density.4 in the model, the sensitivity increased to 64.86 (24/(13+24)) but the accuracy rate slightly decreased to 84.60% ((811+24)/987).

```{r 2.2.10, echo=FALSE, warning = FALSE}
model_cancer7 = glm(cancer ~ recall + age.70plus + menopause.postunknown + density.4, data=brca_new, family=binomial)
yhat_7 = predict(model_cancer7, brca_new, type="response")
prob_cancer = sum(brca_new$cancer == 1)/nrow(brca_new)
yhat_test7 = ifelse(yhat_7 >= prob_cancer, 1, 0)
table(y=brca_new$cancer, yhat=yhat_test7)
```

Since a cancer that is diagnosed at an early stage is more likely to be treated successfully, it is reasonable to argue that the increase in sensitivity overcomes the decrease in the accuracy rates caused by a higher amount of false positive predictions. Hence, our best judgement is that the radiologists should be weighting more heavily patients with 70 years old and above, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status when deciding whether to recall patients.
