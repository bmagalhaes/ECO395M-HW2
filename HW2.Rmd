---
title: "homework1"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
always_allow_html: true
output:
    md_document:
    variant: markdown_github
---
#ECO 395M: Exercise 2

Bernardo Arreal Magalhaes - UTEID ba25727

Adhish Luitel - UTEID al49674

Ji Heon Shim - UTEID js93996

## Exercise 2.2
```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(margins)
library(dplyr)
library(kableExtra)
library(sjPlot)
library(mosaic)

brca = read.csv(url("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW2/master/brca.csv"))
```
This exercise is based on a dataset consisted of 987 screening mammograms administered at a hospital in Seattle, Washington. The goal of the analysis is to evaluate the performance of five different radiologists considering several risk factors.

First, we analyzed the raw data to verify whether each radiologist has a different recall rate or not, and compare precision and error rates. We can observe that, even though radiologist89 has a higher probability of recalling patients, his Type II error rate (not recalling patients that actually have cancer) doesn't substantially differ from radiologist95 and radiologist34, who have the lowest recall rates.

```{r 2.2.1, echo=FALSE}
recall_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(A.recallrate = length(which(recall == 1)) / (length(which(recall == 1))
            + length(which(recall == 0))))

recall_rate = recall_rate[order(-recall_rate$A.recallrate),c(1,2)]
recall_rate = mutate(recall_rate, n = row_number())

cancer_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(B.cancerrate = length(which(recall == 1 & cancer == 1)) / (length(which(recall == 1 & 
            cancer == 1)) + length(which(recall == 1 & cancer == 0))))

error_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(C.error = length(which(recall == 0 & cancer == 1)) / (length(which(recall == 0 & 
            cancer == 1)) + length(which(recall == 0 & cancer == 0))))

radiologist = left_join(recall_rate, cancer_rate, by = "radiologist")
radiologist = left_join(radiologist, error_rate, by = "radiologist")

radiologist_long = radiologist %>%
  gather("Stat", "Value", -radiologist, -n)
radiologist_long = mutate(radiologist_long, Value = Value*100)

radiologist_89 = radiologist_long[which(radiologist_long$n==1), ]

ggplot(data = radiologist_long) + 
  geom_bar(mapping = aes(x=reorder(radiologist, -n), y=Value),
           stat='identity', position ='dodge', fill="lightgray", color="black", alpha=.6, width=.5) + 
  facet_wrap(~Stat) + 
  coord_flip() +
  labs(y="Observed rate (in percentage)", x = "") +
  geom_hline(data= radiologist_89, aes(yintercept=Value), color="red" , linetype = "dashed") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) 
```

However, since each radiologist read the mammograms of a different set of patients, this difference could be explained by the fact that some radiologists might have seen patients whose clinical situation required them to be recalled for further examination.

In order to analyse if, holding patient risk factors equal, some radiologists are more clinically conservative than others in recalling patients, we built two classification models:

```{r 2.2.2, echo=TRUE}
model1 = recall ~ radiologist + age + history + symptoms + menopause + density
model2 = recall ~ (age + history + symptoms + menopause + density) * radiologist
```

The table below shows the Average Marginal Effect (AME) for each radiologist in the two models. We can see that, when using Model 1, radiologist89 is the most conservative - holding patient risk factors equal, the probability of being recalled increases by 5.71 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

When allowing for interactions between radiologist and each control variable as in Model 2, radiologist89 is still the most conservative - holding patient risk factors equal, the probability of being recalled increases by 17.48 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

```{r 2.2.3, echo=FALSE, warning = FALSE}
brca2 = brca
brca2$radiologist = str_replace(brca2$radiologist, "radiologist","")
brca2$age = str_replace(brca2$age, "age","")
brca2$menopause = str_replace(brca2$menopause, "meno","")
brca2$density = str_replace(brca2$density, "density","")

brca_new = dummy.data.frame(brca2, names = c("radiologist", "age","menopause", "density") , sep = ".")
brca_new = subset(brca_new, select = -c(radiologist.13, age.4049,menopause.postHT, density.1))

model_1 = glm(recall ~ . - cancer, data=brca_new, family=binomial)
ame_11 = summary(margins(model_1, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

model_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
              + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
              + radiologist.66 + radiologist.89 + radiologist.95), data=brca_new, family=binomial)
ame_21 = summary(margins(model_2, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

ame_11 = select(ame_11,-c(4:7))
ame_21 = select(ame_21,-c(4:7))

ame = left_join(ame_11, ame_21, by = "factor")
colnames(ame) <- c("","AME","se","AME","se")

kable(ame) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "Model 1" = 2, "Model 2" = 2))
```

Finally, in order to estimate how each radiologist would perform when facing the same set of patients, we used a bootstrap to randomly split the original dataset in a training dataset containing 80% of the observations and a testing a dataset containing 20% of the observations, repeating the proccess 100 times. In each repetition, we fitted both models using the train dataset, and compared each model's predictions when all radiologists, in a hypothetical scenario, analyze the entire test dataset. 

We computed average probability of recall per repetition, and calculated the average of the 100 samples to mitigate the effect of randomization. For both models, radiologist89 has the highest probability of recall, followed by radiologist66, radiologist13, radiologist95 and radiologist 34.


```{r 2.2.4, echo=FALSE, warning = FALSE}
n = nrow(brca_new)

boot1 = do(100)*{
  
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  brca_train = brca_new[train_cases,]
  brca_test = brca_new[test_cases,]
  brca_test = brca_test[rep(seq_len(nrow(brca_test)), 5),]
  brca_test =  brca_test %>%
    mutate(n = row_number()) %>%
    mutate(radiologist.34 = 0) %>%
    mutate(radiologist.66 = 0) %>%
    mutate(radiologist.89 = 0) %>%
    mutate(radiologist.95 = 0)
  brca_test = brca_test %>%
    mutate(radiologist.34 = ifelse(n > nrow(brca_test)/5 & nrow(brca_test)*2/5 >= n, 1, 0)) %>%
    mutate(radiologist.66 = ifelse(n > nrow(brca_test)*2/5 & nrow(brca_test)*3/5 >= n, 1, 0)) %>%
    mutate(radiologist.89 = ifelse(n > nrow(brca_test)*3/5 & nrow(brca_test)*4/5 >= n, 1, 0)) %>%
    mutate(radiologist.95 = ifelse(n > nrow(brca_test)*4/5 & nrow(brca_test) >= n, 1, 0))
 
  # fit to this training set
  glm_1 = glm(recall ~ . - cancer, data=brca_train, family=binomial)
  glm_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
                        + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
                        + radiologist.66 + radiologist.89 + radiologist.95), data=brca_train, family=binomial)
  
  # predict on this testing set
  yhat_test1 = predict(glm_1, brca_test, type="response")
  yhat_test2 = predict(glm_2, brca_test, type="response")
  brca_test = brca_test %>%
    mutate(radiologist = ifelse(radiologist.34 == 1, "radiologist.34",
                         ifelse(radiologist.66 == 1, "radiologist.66",
                         ifelse(radiologist.89 == 1, "radiologist.89",
                         ifelse(radiologist.95 == 1, "radiologist.95", "radiologist.13")))))
  
  brca_test$pred1 = yhat_test1
  brca_test$pred2 = yhat_test2
  recalls_1 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred1))
  recalls_2 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred2))
  recalls_t = rbind(recalls_1, recalls_2)
  recalls = as.data.frame(t(recalls_t))
  recalls = recalls[-1, ]
  colnames(recalls) <- c("M1_radiologist.13", "M1_radiologist.34", "M1_radiologist.66", 
                         "M1_radiologist.89", "M1_radiologist.95", "M2_radiologist.13",
                         "M2_radiologist.34", "M2_radiologist.66", 
                         "M2_radiologist.89", "M2_radiologist.95")
  recalls
  }

boot1[, c(1:10)] = sapply(boot1[, c(1:10)], as.numeric)

radiologist = c("radiologist.13", "radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95")
Model_1 = c(mean(boot1$M1_radiologist.13), mean(boot1$M1_radiologist.34), mean(boot1$M1_radiologist.66),
       mean(boot1$M1_radiologist.89), mean(boot1$M1_radiologist.95))
Model_2 = c(mean(boot1$M2_radiologist.13), mean(boot1$M2_radiologist.34), mean(boot1$M2_radiologist.66),
       mean(boot1$M2_radiologist.89), mean(boot1$M2_radiologist.95))
a = data.frame(radiologist, Model_1,Model_2)

kable(a) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "(Out of sample) Average recall probability" = 2))
```

The results corroborate what we observed in the AME table, that some radiologists are more clinically conservative than others in recalling patients, even holding patient risk factors equal.

After that, we analyzed if the data suggests that radiologists at this hospital should be weighting some clinical risk factors more heavily than they currently are when interpreting mammograms in order to make a decision on whether to recall a patient or not.

We started by stablishing the following baseline model.

```{r 2.2.5, echo=TRUE}
baseline_model = cancer ~ recall
```

Then, we built multiple models adding each of the clinical risk factors to the baseline model.

```{r 2.2.6, echo=TRUE}
baseline_history_model = cancer ~ recall + history
baseline_age_model = cancer ~ recall + age
baseline_symptoms_model = cancer ~ recall + symptoms
baseline_menopause_model = cancer ~ recall + menopause
baseline_density_model = cancer ~ recall + density
```

If the radiologists were appropriately accounting for the clinical risk factors when deciding whether to recall a patient for further examination, we would expect the coefficient associated with the recall variable to capture this effect, and the coefficient of the control variable to be close to zero (so the odds ratio to be close to one). Hence, including this variables in the model shouldn't considerably affect the cancer predictions.

The table below summarizes the estimates from the 6 models. Here, we observe that age 70 or plus, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status are some factors that are increasing the odds of having a cancer, even after medical analysis! Thus there's extra information in the risk factors that the doctors should be weighting more heavily to recall patients than they currently are.

```{r 2.2.7, echo=FALSE, warning = FALSE}
model_cancer = glm(cancer ~ recall, data=brca, family=binomial)
model_cancer2 = glm(cancer ~ recall + history, data=brca, family=binomial)
model_cancer3 = glm(cancer ~ recall + age, data=brca, family=binomial)
model_cancer4 = glm(cancer ~ recall + symptoms, data=brca, family=binomial)
model_cancer5 = glm(cancer ~ recall + menopause, data=brca, family=binomial)
model_cancer6 = glm(cancer ~ recall + density, data=brca, family=binomial)

tab_model(model_cancer, model_cancer2, model_cancer3, model_cancer4, model_cancer5,
          model_cancer6, show.ci = FALSE, show.p = TRUE,
          dv.labels = c("Baseline Model", "+History", "+Age", "+Symptoms",
                        "+Menopause", "+Density"))
```

Considering the results above, we built a new model including those 3 risk factors.

```{r 2.2.8, echo=TRUE}
proposed_model = cancer ~ recall + age.70plus + postmenounknown + density.4
```

In order to assess how well this model performs when predicting cancer status, we need to compare its predictions with the predictions from the baseline model.

The confusion matrix for the baseline model below shows that the doctors are currently predicting cancer with a 59.46% sensitivity (22/(22+15)) and a accuracy rate of 85.71% ((824+22)/987).

```{r 2.2.9, echo=FALSE, warning = FALSE}
xtabs(~cancer + recall, data=brca)
```

Considering the in-sample probability of cancer as a threshold for the fitted values of the proposed model to predict cancer or not, we computed the following confusion matrix. This matrix show that, when including age.70plus, postmenounknown and density.4 in the model, the sensitivity increased to 64.86 (24/(13+24)) and the accuracy rate slightly decreased to 84.60% ((811+24)/987).

```{r 2.2.10, echo=FALSE, warning = FALSE}
model_cancer7 = glm(cancer ~ recall + age.70plus + menopause.postunknown + density.4, data=brca_new, family=binomial)
yhat_7 = predict(model_cancer7, brca_new, type="response")
prob_cancer = sum(brca_new$cancer == 1)/nrow(brca_new)
yhat_test7 = ifelse(yhat_7 >= prob_cancer, 1, 0)
table(y=brca_new$cancer, yhat=yhat_test7)
```

Since a cancer that is diagnosed at an early stage is more likely to be treated successfully, it is reasonable to argue that the increase in sensitivity overcomes the decrease in the accuracy rates caused by a higher amount of false positive predictions. Hence, our best judgement is that the radiologists should be weighting more heavily patients with 70 years old and above, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status when deciding whether to recall patients.

## Exercise 2.3
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(class)
library(FNN)
library(foreach)
library(knitr)
library(ggplot2)

dataset = read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv')
```

In this exercise, we attempt to build the best possible model to predict which Mashable article goes viral. The threshold here for defining a viral article refers to reaching or surpassing the threshold of 1,400 shares. From the summary statistics we can see that we have 39,644 observations in total with 36 variables, except URL since it is redundant for us. 

The dependant variable for us would be the number of shares and we could consider every other variable as independent variable which results in the number of shares. 

We can sort and clean out our data a bit more. Since we don't need the variable 'URL' we will take it out. We will remove the average length of the words in the content ('average_token_length) that is zero, as there should be some words in the article. We will also remove variables like number of words in the article ('n_tokens_content') for the same reason. 

```{r 2.3.1, echo=FALSE}
# Divide groups into 2 and define 'Root Mean Squre Error' function
news_articles = subset(dataset, select = -c(url))
news_zero_content = news_articles[which(news_articles$n_tokens_content==0),]
news_articles = news_articles[-which(news_articles$n_tokens_content==0),]
```

Now we are left with 38,463 observations.

Let's see the data in a histogram.

```{r 2.3.2, echo=FALSE}
hist(news_articles$shares, xlim=c(1,10000), breaks=5000) ; abline(v=1400, col='blue')
hist(log(news_articles$shares), breaks=50) ; abline(v=log(1400), col='blue')
news_articles = mutate(news_articles, logshares = log(shares))
```

Since the data is uniformly distributed. It would be ideal to log of our dependent variable, which is shares. We take log scale of shares (log(shares)) for our linear and logit regression.

Also, we will use the mutate function to create a new variable, log of shares to replace shares with in the dataset. The mutate function first collects various share values into a seperate dataframe.

Now let's get a bit more into data visualization for more insights. 

Now we start to analyze the data. Since our goal is to find a worthwhile relationship between logshares and other variables, we will attempt to create the best possible model for it. First, it makes sense to do a train-test split.  Like always we use 80% of the data to build a model by training and 20% data for testing.

```{r 2.3.4, echo=FALSE}
n = nrow(news_articles)
n_train = round(0.8*n)  
n_test = n - n_train
```

Let's build a linear regression model by hand build and trial. 
We first use the overall error rate to find the best linear model. We will try 3 linear models and use the overall error rate as the metric to find the best model. Model 3 is the primary model to check the performance of the hand-build model. By assessing the relationship of variables, building the model and testing it, we make the second model. Then we build the linear model 1 by removing the less likely variables to get the most accurate linear fit. 

```{r 2.3.5, echo=FALSE}
err_vals = do(250)*{
  
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_articles_train = news_articles[train_cases,]
  news_articles_test = news_articles[test_cases,]
  
  lm1 = lm(logshares ~ num_hrefs + n_tokens_content + num_self_hrefs + average_token_length + 
             num_keywords + data_channel_is_lifestyle + 
             num_imgs * (data_channel_is_bus + data_channel_is_socmed + data_channel_is_world) +
             num_videos * (data_channel_is_entertainment + data_channel_is_bus + 
                        data_channel_is_socmed + data_channel_is_tech + data_channel_is_world) +
             self_reference_avg_sharess * weekday_is_saturday + 
             is_weekend + global_rate_positive_words * is_weekend + 
             title_subjectivity + title_sentiment_polarity, data = news_articles_train)
  
  lm2 =  lm(logshares ~ n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + 
              average_token_length + num_keywords + (num_videos + num_imgs) * 
              (data_channel_is_lifestyle + data_channel_is_entertainment +
                 data_channel_is_bus + data_channel_is_socmed +
                 data_channel_is_tech + data_channel_is_world) +
              self_reference_avg_sharess * (self_reference_min_shares + 
              self_reference_max_shares) + weekday_is_monday + 
              global_rate_positive_words * avg_positive_polarity + 
              global_rate_negative_words * avg_negative_polarity + title_subjectivity +
              title_sentiment_polarity, data = news_articles_train, weekday_is_friday = binomial)
  
  lm3 = lm(logshares ~ (n_tokens_title + n_tokens_content + num_hrefs + 
                      num_self_hrefs + num_imgs + num_videos + 
                      average_token_length + num_keywords + data_channel_is_lifestyle + 
                      data_channel_is_entertainment + data_channel_is_bus + 
                      + data_channel_is_socmed + data_channel_is_tech + 
                      data_channel_is_world + self_reference_avg_sharess + 
                      weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                      weekday_is_thursday + weekday_is_friday + weekday_is_saturday)^2, data=news_articles_train)
  
  yhat_test1 = predict(lm1, news_articles_test)
  yhat_test2 = predict(lm2, news_articles_test)
  yhat_test3 = predict(lm3, news_articles_test)
  
  # confusion rate
  c(conf_rate(news_articles_test$logshares, yhat_test1), conf_rate(news_test$logshares, yhat_test2),
    conf_rate(news_articles_test$logshares, yhat_test3)) %>% round(3)
  
}
colMeans(err_vals)
```

We might take an optional book-keeping step and arrange the test data.

```{r 2.3.6, echo=FALSE}
D_test = arrange(D_test, mileage)
```

After this we seperate the training and testing sets into mileage (X) and outcome (Y)

```{r 2.3.7, echo=FALSE}
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

```

Now let's have a look at the confusion matrix. 

```{r 2.3.8, echo=FALSE}
confusion_matrix = table(y_test, yhat_test)
confusion_matrix
```

Now let's try our hand at the second approach, which is building a classification model. We now build a logit model for classification as the dependent variable here (viral or not) is a binary. As expected, independent variables would be the same as for a linear model. 

```{r 2.3.9, echo=FALSE}
news_articles = mutate(news_articles, viral = ifelse(shares > 1400,1,0))
errs_vals = do(250) * {
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news_articles[train_cases,]
  news_test = news_articles[test_cases,]
  
  logit_model = glm(viral ~  n_tokens_content + num_hrefs + num_self_hrefs + average_token_length + 
                  num_keywords + data_channel_is_lifestyle + num_videos * 
                  (data_channel_is_bus + data_channel_is_socmed + data_channel_is_world) + video * 
                  (data_channel_is_entertainment + weekday_is_monday +
                     data_channel_is_tech + data_channel_is_world) + self_reference_max_shares + is_weekend + 
                    global_rate_positive_words * 
                  avg_positive_polarity + avg_negative_polarity +
                  title_subjectivity + title_sentiment_polarity, data = news_train, family = 'binomial')
  phat_logit = predict(logit_m, news_test, type = 'response')
  yhat_logit = ifelse(phat_logit>0.5, 1, 0)
  ct_lg = table(news_test$viral, yhat_logit)
  
  # linear
  finalin = lm(logshares ~ n_tokens_content + num_hrefs + num_self_hrefs + average_token_length + 
             num_keywords + data_channel_is_lifestyle + num_imgs * 
             (data_channel_is_bus + data_channel_is_socmed + data_channel_is_world) + video * 
             (data_channel_is_entertainment + global_rate_positive_words + global_rate_negative_words + data_channel_is_socmed +
                data_channel_is_tech + data_channel_is_world) +  self_reference_avg_sharess * 
             self_reference_max_shares + is_weekend + global_rate_positive_words * 
             avg_positive_polarity + avg_negative_polarity +
             title_subjectivity + title_sentiment_polarity, data = news_train)
  yhatF = predict(lmF, news_test)
  ct_lm = conf_table(news_test$logshares, yhatF)
```

Our results are 

```{r 2.3.10, echo=FALSE}
((1-sum(diag(ct_lg))/sum(ct_lg)), (1-sum(diag(ct_lm))/sum(ct_lm)),
    ct_lg[2,2]/sum(ct_lg[2,]), ct_lm[2,2]/sum(ct_lm[2,]),
    ct_lg[1,2]/sum(ct_lg[1,]), ct_lm[1,2]/sum(ct_lm[1,]))
} 
errMean = colMeans(errs_vals) %>% round(3)
err = matrix(errMean, nrow = 2, dimnames = 
               list(c("Classification Model", "Linear Model"), 
                    c("Overall Error", "True Positive", "False Positive"))) 

kable(err) %>% kable_styling("striped")
```

Confusion matrix of our best model

```{r 2.3.11, echo=FALSE}
confusion = table(y = n_train$viral, yhat_train)
confusion
```

In-sample accuracy

```{r 2.3.12, echo=FALSE}
probhat_test = predict(finalin, news_test=spamtest)
yhat_train = ifelse(predict(finalin) >= 0.75, 1, 0)
table(y=news_test$logshares, yhat=yhatF)
```

KNN

```{r 2.3.13, echo=FALSE}
KNN_result <- data.frame(K=c(), rsme=c())
k_grid = seq(3, 150, by=20)
for(v in k_grid){
  avgrmse = KNN_C_error(data_X = X,data_y = Y, K = v,Ntimes = 5)
  KNN_result <- rbind(KNN_result,c(v,avgrmse))
}
colnames(KNN_result) <- c("K","AVG_RMSE")
ggplot(data = KNN_result, aes(x = K, y = AVG_RMSE)) + 
  geom_point(shape = "O") +
  geom_line(col = "red")


obj_Y = subset(news_articles,select = c(viral))

KNN_result <- data.frame(K=c(), rsme=c())
k_grid = seq(53, 250, by=30)
for(v in k_grid){
  avgrmse = KNN_C_error_linear(data_X = x,data_y = Y,obj_y = obj_Y, K = v,threshold = 1400,Ntimes = 5)
  KNN_result <- rbind(KNN_result,c(v,avgrmse))
}
colnames(KNN_result) <- c("K","AVG_RMSE")
ggplot(data = KNN_result, aes(x = K, y = AVG_RMSE)) + 
  geom_point(shape = "O") +
  geom_line(col = "red")
```
Again, we formalize training and testing sets. 