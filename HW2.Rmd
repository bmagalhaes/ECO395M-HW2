---
title: "homework1"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
always_allow_html: true
output:
    md_document:
    variant: markdown_github
---
#ECO 395M: Exercise 2

Bernardo Arreal Magalhaes - UTEID ba25727

Adhish Luitel - UTEID al49674

Ji Heon Shim - UTEID js93996

## Exercise 2.1
```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(margins)
library(dplyr)
library(kableExtra)
library(sjPlot)
library(mosaic)
library(FNN)
library(foreach)
library(jtools)
data(SaratogaHouses)
```

In this exercise, we analyzed a data set on house prices in Saratoga, NY, in order to provide the local tax authority a predictive model to estimate properties market values.

We started by hand-building five models for price in order to find out the best one which outperforms the "medium" model that we considered in class.

```{r 2.1.1, echo= TRUE, warning = FALSE}
model1= price ~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + sewer + waterfront + newConstruction + centralAir 
model2= price~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + centralAir 
model3= price~ (lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + heating + fuel + centralAir)^2 
model4= price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir
model5= price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir

model_medium = price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir
```

The main features of our models are:

Model 1 : include all main effects except roooms
(excluded due to some degree of collinearity with bedrooms and bathrooms)

Model 2: simplify model1 by reducing some variables(-sewer-waterfront-newConstruction)

Model 3: add all the interactions on model 2

Model 4: allow only some interactions on model 2

Model 5: a polynomial model by adding age^2 on model 4

Model_medium: baseline model with 11 main effects
             
```{r 2.1.2, echo=FALSE}
# make a new variable age2=(age)^2
SaratogaHouses <- mutate(SaratogaHouses, age2=(age)^2)

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
```

In order to measure performances of each model, we randomly splitted the data into training-test sets (with the training set containing 80% of the ovservations and the test set containing the other 20%) 100 times and calcaulated the average values of out-of-sample RMSE for each model by comparing the fitted values with the observed values in each repetition.

```{r 2.1.3, echo=FALSE, warning=FALSE}
rmse_check = do(100)*{# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

#Hand-build some models and fit into training data

# model1: use all possible variables except rooms and age2 without interactions
# we get rid of "rooms" because of collinearity, rooms=bathrooms+bedrooms)
model1= lm(price ~ .-rooms-age2, data=saratoga_train)

# model2: simplify model1 by deleting some variables that look unimportant
# sewer(unimportant), waterfront, newConstruction(not enough samples with value of 1)
model2= lm(price~ .-rooms-sewer-waterfront-newConstruction-age2, data=saratoga_train)

# model3 : add all the interactions on model2
model3= lm(price~ (.-rooms-sewer-waterfront-newConstruction-age2)^2, data=saratoga_train)

# model4: allow only some interactions
model4= lm(price~ lotSize + age +pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# add a new variables for polynomial regression on model4
# we added age^2 because it is likely that house price rapidly goes down as it becomes aged

model5= lm(price~ lotSize + age + age2 + pctCollege * landValue + livingArea * (bedrooms + bathrooms) + fireplaces + heating + fuel + centralAir, data=saratoga_train)

# our baseline model, medium
model_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# Predictions out of sample
yhat_test1 = predict(model1, saratoga_test)
yhat_test2 = predict(model2, saratoga_test)
yhat_test3 = predict(model3, saratoga_test)
yhat_test4 = predict(model4, saratoga_test)
yhat_test5 = predict(model5, saratoga_test)
yhat_test_medium = predict(model_medium, saratoga_test)

c(rmse(saratoga_test$price, yhat_test1), rmse(saratoga_test$price, yhat_test2), rmse(saratoga_test$price, yhat_test3), rmse(saratoga_test$price, yhat_test4), rmse(saratoga_test$price, yhat_test5), rmse(saratoga_test$price, yhat_test_medium))}

rmse_result= data.frame("model1"= colMeans(rmse_check[1]), "model2"=colMeans(rmse_check[2]), "model3"=colMeans(rmse_check[3]), "model4"=colMeans(rmse_check[4]), "model5"=colMeans(rmse_check[5]), "model medium"=colMeans(rmse_check[6]), row.names="AVG RMSE")

kable(rmse_result) %>% kable_styling("striped")

```
  
The best model turned out to be model 1 with the least out-of-sample RMSE value. The table below show the summary statistics for this model.

```{r 2.1.4, echo=TRUE}
summ(model1)
```

In order to assess which variable is the strongest driver of house prices (SO how much it improves the out-of-sample RMSE when it is included in the model), we tested multiple scenarios where we excluded each of the variables from model 1.

Than, we verified how much the out-of-sample RMSE increases when we exclude each certain variable, and get the average RMSE by doing Monte Carlo simulation on different training-test sets (80%-20%) for 100 times.

As we can observe from the table below, "landValue" variable seems to be the strongest drive of house prices. This result might be caused because land values are already included in house prices(House price = Land value + Pure house value), so they are strongly related to each other.

```{r 2.1.5, echo=FALSE, warning=FALSE}
rmse_check2 = do(100)*{# Split into training and testing sets
train_cases2 = sample.int(n, n_train, replace=FALSE)
test_cases2 = setdiff(1:n, train_cases)
saratoga_train2 = SaratogaHouses[train_cases2,]
saratoga_test2 = SaratogaHouses[test_cases2,]

#Hand-build some models and fit into training data

# model1_wo_lotSize: without lotSize
model1_wo_lotSize= lm(price ~ .-rooms-age2-lotSize, data=saratoga_train2)

# model1_wo_age: without age
model1_wo_age= lm(price ~ .-rooms-age2-age, data=saratoga_train2)

# model1_wo_landValue: without landValue
model1_wo_landValue= lm(price ~ .-rooms-age2-landValue, data=saratoga_train2)

# model1_wo_livingArea: without livingArea
model1_wo_livingArea= lm(price ~ .-rooms-age2-livingArea, data=saratoga_train2)

# model1_wo_pctCollege: without pctCollege
model1_wo_pctCollege= lm(price ~ .-rooms-age2-pctCollege, data=saratoga_train2)

# model1_wo_bedrooms: without bedrooms
model1_wo_bedrooms= lm(price ~ .-rooms-age2-bedrooms, data=saratoga_train2)

# model1_wo_fireplaces: without fireplaces
model1_wo_fireplaces= lm(price ~ .-rooms-age2-fireplaces, data=saratoga_train2)

# model1_wo_bathrooms: without bathrooms
model1_wo_bathrooms= lm(price ~ .-rooms-age2-bathrooms, data=saratoga_train2)

# model1_wo_heating: without heating
model1_wo_heating= lm(price ~ .-rooms-age2-heating, data=saratoga_train2)

# model1_wo_fuel: without fuel
model1_wo_fuel= lm(price ~ .-rooms-age2-fuel, data=saratoga_train2)

# model1_wo_sewer: without sewer
model1_wo_sewer= lm(price ~ .-rooms-age2-sewer, data=saratoga_train2)

# model1_wo_waterfront: without waterfront
model1_wo_waterfront= lm(price ~ .-rooms-age2-waterfront, data=saratoga_train2)

# model1_wo_newConstruction: without newConstruction
model1_wo_newConstruction= lm(price ~ .-rooms-age2-newConstruction, data=saratoga_train2)

# model1_wo_centralAir: without centralAir
model1_wo_centralAir= lm(price ~ .-rooms-age2-centralAir, data=saratoga_train2)

# Predictions out of sample
yhat_test_lotSize = predict(model1_wo_lotSize, saratoga_test2)
yhat_test_age = predict(model1_wo_age, saratoga_test2)
yhat_test_landValue = predict(model1_wo_landValue, saratoga_test2)
yhat_test_livingArea = predict(model1_wo_livingArea, saratoga_test2)
yhat_test_pctCollege = predict(model1_wo_pctCollege, saratoga_test2)
yhat_test_bedrooms = predict(model1_wo_bedrooms, saratoga_test2)
yhat_test_fireplaces = predict(model1_wo_fireplaces, saratoga_test2)
yhat_test_bathrooms = predict(model1_wo_bathrooms, saratoga_test2)
yhat_test_heating = predict(model1_wo_heating, saratoga_test2)
yhat_test_fuel = predict(model1_wo_fuel, saratoga_test2)
yhat_test_sewer = predict(model1_wo_sewer, saratoga_test2)
yhat_test_waterfront = predict(model1_wo_waterfront, saratoga_test2)
yhat_test_newConstruction = predict(model1_wo_newConstruction, saratoga_test2)
yhat_test_centralAir = predict(model1_wo_centralAir, saratoga_test2)

c(rmse(saratoga_test2$price, yhat_test_lotSize), rmse(saratoga_test2$price, yhat_test_age), rmse(saratoga_test2$price, yhat_test_landValue), rmse(saratoga_test2$price, yhat_test_livingArea), rmse(saratoga_test2$price, yhat_test_pctCollege), rmse(saratoga_test2$price, yhat_test_bedrooms), rmse(saratoga_test2$price, yhat_test_fireplaces), rmse(saratoga_test2$price, yhat_test_bathrooms), rmse(saratoga_test2$price, yhat_test_heating), rmse(saratoga_test2$price, yhat_test_fuel), rmse(saratoga_test2$price, yhat_test_sewer), rmse(saratoga_test2$price, yhat_test_waterfront), rmse(saratoga_test2$price, yhat_test_newConstruction), rmse(saratoga_test2$price, yhat_test_centralAir))}

rmse_result2= data.frame("model wo lotSize"= colMeans(rmse_check2[1]), "model wo age"=colMeans(rmse_check2[2]), "model wo landValue"=colMeans(rmse_check2[3]), "model wo livingArea"=colMeans(rmse_check2[4]), "model wo pctCollege"=colMeans(rmse_check2[5]), "model wo bedrooms"=colMeans(rmse_check2[6]), "model wo fireplaces"= colMeans(rmse_check2[7]), "model wo bathrooms"=colMeans(rmse_check2[8]), "model wo heating"=colMeans(rmse_check2[9]), "model wo fuel"=colMeans(rmse_check2[10]), "model wo sewer"=colMeans(rmse_check2[11]), "model wo waterfront"=colMeans(rmse_check2[12]), "model wo newConstruction"=colMeans(rmse_check2[13]), "model wo centralAir"=colMeans(rmse_check2[14]), row.names="AVG RMSE")

rmse_result2_t = t(rmse_result2)

kable(rmse_result2_t) %>% kable_styling("striped")
```

Now, we built a nonparametic KNN model to compare with our linear model and figure out which one performs better. By using the same train and test sets that we used in our linear regression, the result shows that whatever value K may have, the knn model is unlikely to perform better than our linear model.

In the graph below, the horizontal red line shows the out-of-sample RMSE of our linear model. We can see that all the RMSEs of the knn model in accordance with k values are plotted above the red line.  And the table below suggests that the minimum RMSE value of knn model is still bigger than our best-fit linear model. 

```{r 2.1.6, echo=FALSE}
# construct the training and test-set feature matrices
Xtrain= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train)
Xtest = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test)

# training and testing set responses 
ytrain = saratoga_train$price
ytest = saratoga_test$price

# rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale= scale_train)
K= 10

# fit the model
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)

k_grid = seq(1,50, by=1)
rmse_grid = foreach(K= k_grid, .combine='c') %do% {
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)
rmse(ytest, knn_model$pred)
}

ggplot() + 
  geom_point(mapping = aes(x = k_grid, y = rmse_grid), color="black") + 
  geom_hline(yintercept = rmse(ytest, yhat_test1), color = "red", size=1) +
  labs(x = "K", y = "RMSE") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank()) +
  scale_y_continuous(breaks=c(65000, 70000, 75000, 80000, 85000))

K= which.min(rmse_grid)
modelcompare = data.frame('Kmin'=K, "knn model RMSE" = rmse(ytest, knn_model$pred), "linear model RMSE" = rmse(ytest, yhat_test1))
kable(modelcompare) %>% kable_styling("striped", full_width=F)
```

Since randomness plays a role due to the particular choice of data points that end up in our train/test split, we addressed that issue by running Monte-Carlo simulation again using random train/test split for 100 times, and compared the minimum RMSE of knn model with RMSE of our linear model.
As a result, we can see that our linear model outperforms the knn model.

```{r 2.1.7, echo=FALSE}
rmse_check3 = do(100)*{# Split into training and testing sets
train_cases3 = sample.int(n, n_train, replace=FALSE)
test_cases3 = setdiff(1:n, train_cases3)
saratoga_train3 = SaratogaHouses[train_cases3,]
saratoga_test3 = SaratogaHouses[test_cases3,]

# construct the training and test-set feature matrices
Xtrain3= model.matrix(~ .-(price + rooms + age2)-1, data=saratoga_train3)
Xtest3 = model.matrix(~ .-(price + rooms + age2)-1, data= saratoga_test3)

# training and testing set responses 
ytrain3 = saratoga_train3$price
ytest3 = saratoga_test3$price

# rescale
scale_train3 = apply(Xtrain3, 2, sd)
Xtilde_train3 = scale(Xtrain3, scale = scale_train3)
Xtilde_test3 = scale(Xtest3, scale= scale_train3)
K3=10

# fit the model
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)


k_grid3 = seq(1,50, by=1)
rmse_grid3 = foreach(K3= k_grid3, .combine='c') %do% {
knn_model3 = knn.reg(Xtilde_train3, Xtilde_test3, ytrain3, k=K3)
rmse(ytest3, knn_model3$pred)}
min(rmse_grid3)
}


modelcompare2 = data.frame("knn model"= colMeans(rmse_check3), "linear model"= colMeans(rmse_check[1]), row.names="Average RMSE")
kable(modelcompare2) %>% kable_styling("striped")

```

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(margins)
library(dplyr)
library(kableExtra)
library(sjPlot)
library(mosaic)

brca = read.csv(url("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW2/master/brca.csv"))
```

## Exercise 2.2

This exercise is based on a dataset consisting of 987 screening mammograms administered at a hospital in Seattle, Washington. The goal of the analysis is to evaluate the performance of five different radiologists considering several risk factors.

First, we analyzed the raw data to verify whether each radiologist has a different recall rate (A.recallrate) or not, and compare precision (B.cancerrate) and false negative (C.false_negative) rates. We can observe that, even though radiologist89 has a higher probability of recalling patients, his false negative error rate (not recalling patients that actually have cancer) doesn't substantially differ from radiologist95 and radiologist34, who have the lowest recall rates.

```{r 2.2.1, echo=FALSE}
recall_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(A.recallrate = length(which(recall == 1)) / (length(which(recall == 1))
            + length(which(recall == 0))))

recall_rate = recall_rate[order(-recall_rate$A.recallrate),c(1,2)]
recall_rate = mutate(recall_rate, n = row_number())

cancer_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(B.cancerrate = length(which(recall == 1 & cancer == 1)) / (length(which(recall == 1 & 
            cancer == 1)) + length(which(recall == 1 & cancer == 0))))

error_rate = brca %>%
  group_by(radiologist)  %>%
  summarize(C.false_negative = length(which(recall == 0 & cancer == 1)) / (length(which(recall == 0 & 
            cancer == 1)) + length(which(recall == 0 & cancer == 0))))

radiologist = left_join(recall_rate, cancer_rate, by = "radiologist")
radiologist = left_join(radiologist, error_rate, by = "radiologist")

radiologist_long = radiologist %>%
  gather("Stat", "Value", -radiologist, -n)
radiologist_long = mutate(radiologist_long, Value = Value*100)

radiologist_89 = radiologist_long[which(radiologist_long$n==1), ]

ggplot(data = radiologist_long) + 
  geom_bar(mapping = aes(x=reorder(radiologist, -n), y=Value),
           stat='identity', position ='dodge', fill="lightgray", color="black", alpha=.6, width=.5) + 
  facet_wrap(~Stat) + 
  coord_flip() +
  labs(y="Observed rate (in percentage)", x = "") +
  geom_hline(data= radiologist_89, aes(yintercept=Value), color="red" , linetype = "dashed") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) 
```

However, since each radiologist read the mammograms of a different set of patients, this difference could be explained by the fact that some radiologists might have seen patients whose clinical situation required them to be recalled for further examination.

In order to analyse if, holding patient risk factors equal, some radiologists are more clinically conservative than others in recalling patients, we built two classification models:

```{r 2.2.2, echo=TRUE}
model1 = recall ~ radiologist + age + history + symptoms + menopause + density
model2 = recall ~ (age + history + symptoms + menopause + density) * radiologist
```

The table below shows the Average Marginal Effect (AME) for each radiologist in the two models. We can see that, when using Model 1, radiologist89 is the most conservative - holding patient risk factors equal, the probability of being recalled increases by 5.71 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

When allowing for interactions between radiologist and each control variable as in Model 2, radiologist89 is still the most conservative - holding patient risk factors equal, the probability of being recalled increases by 17.48 percentage points when radiologist89 is the one reading the mammogram compared to the baseline radiologist13.

```{r 2.2.3, echo=FALSE, warning = FALSE}
brca2 = brca
brca2$radiologist = str_replace(brca2$radiologist, "radiologist","")
brca2$age = str_replace(brca2$age, "age","")
brca2$menopause = str_replace(brca2$menopause, "meno","")
brca2$density = str_replace(brca2$density, "density","")

brca_new = dummy.data.frame(brca2, names = c("radiologist", "age","menopause", "density") , sep = ".")
brca_new = subset(brca_new, select = -c(radiologist.13, age.4049,menopause.postHT, density.1))

model_1 = glm(recall ~ . - cancer, data=brca_new, family=binomial)
ame_11 = summary(margins(model_1, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

model_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
              + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
              + radiologist.66 + radiologist.89 + radiologist.95), data=brca_new, family=binomial)
ame_21 = summary(margins(model_2, variables = list("radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95"), vce = "bootstrap"))

ame_11 = select(ame_11,-c(4:7))
ame_21 = select(ame_21,-c(4:7))

ame = left_join(ame_11, ame_21, by = "factor")
colnames(ame) <- c("","AME","se","AME","se")

kable(ame) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "Model 1" = 2, "Model 2" = 2))
```

Finally, in order to estimate how each radiologist would perform when facing the same set of patients, we used a bootstrap to randomly split the original dataset in a training dataset containing 80% of the observations and a testing a dataset containing 20% of the observations, repeating the proccess 100 times. In each repetition, we estimated both models using the train dataset, and compared each model's predictions when all radiologists, in a hypothetical scenario, analyze the entire test dataset. 

We computed average probability of recall per repetition, and calculated the average of the 100 samples to mitigate the effect of randomization. For both models, radiologist89 has the highest probability of recall, followed by radiologist66, radiologist13, radiologist95 and radiologist 34.

```{r 2.2.4, echo=FALSE, warning = FALSE}
n = nrow(brca_new)

boot1 = do(100)*{
  
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  brca_train = brca_new[train_cases,]
  brca_test = brca_new[test_cases,]
  brca_test = brca_test[rep(seq_len(nrow(brca_test)), 5),]
  brca_test =  brca_test %>%
    mutate(n = row_number()) %>%
    mutate(radiologist.34 = 0) %>%
    mutate(radiologist.66 = 0) %>%
    mutate(radiologist.89 = 0) %>%
    mutate(radiologist.95 = 0)
  brca_test = brca_test %>%
    mutate(radiologist.34 = ifelse(n > nrow(brca_test)/5 & nrow(brca_test)*2/5 >= n, 1, 0)) %>%
    mutate(radiologist.66 = ifelse(n > nrow(brca_test)*2/5 & nrow(brca_test)*3/5 >= n, 1, 0)) %>%
    mutate(radiologist.89 = ifelse(n > nrow(brca_test)*3/5 & nrow(brca_test)*4/5 >= n, 1, 0)) %>%
    mutate(radiologist.95 = ifelse(n > nrow(brca_test)*4/5 & nrow(brca_test) >= n, 1, 0))
 
  # fit to this training set
  glm_1 = glm(recall ~ . - cancer, data=brca_train, family=binomial)
  glm_2 = glm(recall ~ (history + symptoms + age.5059 + age.6069 + age.70plus + menopause.postNoHT
                        + menopause.postunknown + menopause.pre + density.2 + density.3 + density.4)*(radiologist.34
                        + radiologist.66 + radiologist.89 + radiologist.95), data=brca_train, family=binomial)
  
  # predict on this testing set
  yhat_test1 = predict(glm_1, brca_test, type="response")
  yhat_test2 = predict(glm_2, brca_test, type="response")
  brca_test = brca_test %>%
    mutate(radiologist = ifelse(radiologist.34 == 1, "radiologist.34",
                         ifelse(radiologist.66 == 1, "radiologist.66",
                         ifelse(radiologist.89 == 1, "radiologist.89",
                         ifelse(radiologist.95 == 1, "radiologist.95", "radiologist.13")))))
  
  brca_test$pred1 = yhat_test1
  brca_test$pred2 = yhat_test2
  recalls_1 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred1))
  recalls_2 = brca_test %>%
    group_by(radiologist)  %>%
    summarize(recalls = mean(pred2))
  recalls_t = rbind(recalls_1, recalls_2)
  recalls = as.data.frame(t(recalls_t))
  recalls = recalls[-1, ]
  colnames(recalls) <- c("M1_radiologist.13", "M1_radiologist.34", "M1_radiologist.66", 
                         "M1_radiologist.89", "M1_radiologist.95", "M2_radiologist.13",
                         "M2_radiologist.34", "M2_radiologist.66", 
                         "M2_radiologist.89", "M2_radiologist.95")
  recalls
  }

boot1[, c(1:10)] = sapply(boot1[, c(1:10)], as.numeric)

radiologist = c("radiologist.13", "radiologist.34", "radiologist.66", "radiologist.89", "radiologist.95")
Model1 = c(mean(boot1$M1_radiologist.13), mean(boot1$M1_radiologist.34), mean(boot1$M1_radiologist.66),
       mean(boot1$M1_radiologist.89), mean(boot1$M1_radiologist.95))
Model2 = c(mean(boot1$M2_radiologist.13), mean(boot1$M2_radiologist.34), mean(boot1$M2_radiologist.66),
       mean(boot1$M2_radiologist.89), mean(boot1$M2_radiologist.95))
a = data.frame(radiologist, Model1,Model2)

kable(a) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "(Out of sample) Average recall probability" = 2))
```

The results corroborate what we observed in the AME table, that some radiologists are more clinically conservative than others in recalling patients, even holding patient risk factors equal.

After that, we analyzed if the data suggests that radiologists at this hospital should be weighting some clinical risk factors more heavily than they currently are when interpreting mammograms in order to make a decision on whether to recall a patient or not.

We started by stablishing the following baseline model.

```{r 2.2.5, echo=TRUE}
baseline_model = cancer ~ recall
```

Then, we built multiple models adding each of the clinical risk factors to the baseline model to evaluate the results separately.

```{r 2.2.6, echo=TRUE}
baseline_history_model = cancer ~ recall + history
baseline_age_model = cancer ~ recall + age
baseline_symptoms_model = cancer ~ recall + symptoms
baseline_menopause_model = cancer ~ recall + menopause
baseline_density_model = cancer ~ recall + density
```

If the radiologists were appropriately accounting for the clinical risk factors when deciding whether to recall a patient for further examination, we would expect the coefficient associated with the recall variable to capture this effect, and the coefficient of the control variable to be close to zero (so the odds ratio to be close to one). Hence, including this variables in the model shouldn't considerably affect the cancer predictions.

The table below summarizes the estimates from the 6 models. Here, we observe that age 70 plus, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status are some factors that are increasing the odds of having a cancer, even after medical analysis! Thus there's extra information in the risk factors that the doctors should be weighting more heavily to recall patients than they currently are.

```{r 2.2.7, echo=FALSE, warning = FALSE}
model_cancer = glm(cancer ~ recall, data=brca, family=binomial)
model_cancer2 = glm(cancer ~ recall + history, data=brca, family=binomial)
model_cancer3 = glm(cancer ~ recall + age, data=brca, family=binomial)
model_cancer4 = glm(cancer ~ recall + symptoms, data=brca, family=binomial)
model_cancer5 = glm(cancer ~ recall + menopause, data=brca, family=binomial)
model_cancer6 = glm(cancer ~ recall + density, data=brca, family=binomial)

tab_model(model_cancer, model_cancer2, model_cancer3, model_cancer4, model_cancer5,
          model_cancer6, show.ci = FALSE, show.p = TRUE,
          dv.labels = c("Baseline Model", "+History", "+Age", "+Symptoms",
                        "+Menopause", "+Density"))
```

Considering the results above, we built a new model including those 3 risk factors.

```{r 2.2.8, echo=TRUE}
proposed_model = cancer ~ recall + age.70plus + postmenounknown + density.4
```

In order to assess how well this model performs when predicting cancer status, we need to compare its predictions with the predictions from the baseline model.

The confusion matrix for the baseline model below shows that the doctors are currently predicting cancer with a 59.46% sensitivity (22/(22+15)) and a accuracy rate of 85.71% ((824+22)/987).

```{r 2.2.9, echo=FALSE, warning = FALSE}
xtabs(~cancer + recall, data=brca)
```

Considering the in-sample probability of cancer as a threshold for the fitted values of the proposed model to predict cancer or not, we computed the following confusion matrix. This matrix shows that, when including age.70plus, postmenounknown and density.4 in the model, the sensitivity increased to 64.86 (24/(13+24)) but the accuracy rate slightly decreased to 84.60% ((811+24)/987).

```{r 2.2.10, echo=FALSE, warning = FALSE}
model_cancer7 = glm(cancer ~ recall + age.70plus + menopause.postunknown + density.4, data=brca_new, family=binomial)
yhat_7 = predict(model_cancer7, brca_new, type="response")
prob_cancer = sum(brca_new$cancer == 1)/nrow(brca_new)
yhat_test7 = ifelse(yhat_7 >= prob_cancer, 1, 0)
table(y=brca_new$cancer, yhat=yhat_test7)
```

Since the medical literature suggests that a cancer that is diagnosed at an early stage is more likely to be treated successfully, it is reasonable to argue that the increase in sensitivity overcomes the decrease in the accuracy rates caused by a higher amount of false positive predictions. Hence, our best judgement is that the radiologists should be weighting more heavily patients with 70 years old and above, density 4 (extremely dense) and post-menopausal/unknown hormone-therapy status when deciding whether to recall patients.

```{r setup3, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
library(foreach)
library(knitr)
library(kableExtra)
library(ggplot2)

dataset = read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv')

```

# Exercise 2.3

In this exercise, we analyzed data from 39,797 online articles published by Mashable during 2013 and 2014, in order to build a model to predict which article goes viral. The criteria used for defining an article as viral is if it surpasses the threshold of 1,400 shares. For each online article, we have 36 variables that can be included in the model (excluding URL since it is redundant for this analysis). 

We started by plotting a histogram to visualize the frequency distribution of the shares per article. 

```{r 2.3.1, echo=FALSE, warning = FALSE}
ggplot(data=dataset, aes(x = shares)) +
  geom_histogram(binwidth = 200, fill="lightgray", color="black", alpha=.6) +
  geom_vline(xintercept = 1400 , color="red" , linetype = "dashed") +
  labs(title = 'Shares Histogram', x = "# Shares", y = "Frequency")+
  xlim(0, 10000)+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), panel.grid.major.y = element_line(colour = "grey"))
```

If we observe the histogram of the number of shares, we can see that the data is skewed towards the lower end. It shows that although there are a broad range of articles that have well exceded 7,000 shares, a large portion of Mashable's articles are in the 800-1,200 shares range. A majority being slightly below our 1,400 shares threshold.

As the variation of the shared articles is very broad, ranging from almost 0 shares to close to 10,000 shares, it made sense to log scale the response variable shares in order to compress down our data for uniformity.

```{r 2.3.2, echo=FALSE}
ggplot(data=dataset, aes(x = log(shares))) +
  geom_histogram(binwidth = 0.275, fill="lightgray", color="black", alpha=.6) +
  geom_vline(xintercept = log(1400) , color="red" , linetype = "dashed") +
  labs(title = 'Log Shares Histogram', x = "Log Shares", y = "Frequency")+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), panel.grid.major.y = element_line(colour = "grey"))
```

Since the data is more uniformly distributed in the log scale, we decided to use the transformed variable log(shares) in our predictive models.

We started the model selection by building a baseline model including all the available features as explanatory variables. So for our first linear model, we fitted all 36 available variables (except for URL) to make further assessments. 

```{r 2.3.3, echo=TRUE}
lm1 = log(shares) ~ n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + num_imgs +               num_videos + average_token_length + num_keywords + data_channel_is_lifestyle +                 data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_max_shares + self_reference_avg_sharess + weekday_is_monday +                   weekday_is_tuesday + weekday_is_wednesday + weekday_is_thursday +                              weekday_is_friday + weekday_is_saturday + weekday_is_sunday + is_weekend +                     global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +               min_positive_polarity + max_positive_polarity + avg_negative_polarity +                        min_negative_polarity + max_negative_polarity + title_subjectivity +                           title_sentiment_polarity + abs_title_sentiment_polarity
```

The table below summarizes the coefficients associated with each variable from this model.

```{r 2.3.4, echo=FALSE}
lm1 = lm(log(shares) ~ . - url, data=dataset)
summ(lm1)
```

For our linear models 2 and 3, we removed the variables that were droppef from lm1 due to multicollinearity or that yielded a high p-value such as the number of words in the content (n_tokens_content), maximum share of referenced articles (self_reference_max_shares), maximum polarity of positive words (max_positive_polarity), minimum polarity of negative words (min_negative_polarity), and the dummy variables indicating if the article was posted in a sunday (weekday_is_sunday) or in the weekend (is_weekend).

Finally, for our linear model 4 we used a stepwise selection (starting from lm2 - with up to 10 steps) and, in order to determine the subset of variables to include, we chose the one that yields the minimum Akaike Information Criteria (AIC).

```{r 2.3.5, echo=TRUE}
lm2 = log(shares) ~ n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + num_videos +            average_token_length + num_keywords + data_channel_is_lifestyle +                              data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday +                          weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday +         global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +              min_positive_polarity + avg_negative_polarity + max_negative_polarity +                        title_subjectivity + title_sentiment_polarity + abs_title_sentiment_polarity

lm3 = log(shares) ~ (n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + num_videos +            average_token_length + num_keywords + data_channel_is_lifestyle +                              data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday +                          weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday +         global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +              min_positive_polarity + avg_negative_polarity + max_negative_polarity +                        title_subjectivity + title_sentiment_polarity + abs_title_sentiment_polarity)^2

lm4 = log(shares) ~ n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + 
      num_videos + average_token_length + num_keywords + data_channel_is_lifestyle + 
      data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + 
      data_channel_is_tech + data_channel_is_world + self_reference_min_shares + 
      self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + 
      weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + 
      weekday_is_saturday + global_rate_positive_words + global_rate_negative_words + 
      avg_positive_polarity + min_positive_polarity + avg_negative_polarity + 
      max_negative_polarity + title_subjectivity + title_sentiment_polarity + 
      abs_title_sentiment_polarity + self_reference_min_shares:self_reference_avg_sharess + 
      num_self_hrefs:num_imgs + num_hrefs:data_channel_is_bus + 
      data_channel_is_socmed:avg_positive_polarity + num_keywords:global_rate_negative_words + 
      n_tokens_title:num_self_hrefs + num_self_hrefs:global_rate_positive_words + 
      num_imgs:data_channel_is_world + data_channel_is_entertainment:min_positive_polarity + 
      num_hrefs:data_channel_is_tech
```

In order to determine the most accurate model which accounts the relevant variables to predict how articles reach the 1,400 shares threshold, we utilized a bootstrap to randomly split the original dataset in a training dataset containing 80% of the observations and a testing a dataset containing the remaining 20%. We then repeated this proccess 100 times and, for each repetition, we estimated our four linear models using the train dataset, and fitted the values in the test dataset to compare each model's prediction error rate. 

The table shown below summarizes the average error rate for each of the four models. The results were averaged over 100 random train/test split samples in order to mollify the irregularities that could arise from randomization of samples. We can infer from the results that, on average, the third linear model yields the most accurate predictions.

```{r 2.3.6, echo=FALSE, warning=FALSE}
news_loop = subset(dataset, select = -c(url))
news_loop$shares = log(news_loop$shares)

error_rate = function(y, yhat) {
  y_test = ifelse(y>log(1400), 1, 0)
  y_hat = ifelse(yhat>log(1400), 1, 0)
  sum(y_hat != y_test)/length(y)
}  

n = nrow(news_loop)
n_train = round(0.8*n)
n_test = n - n_train

err_vals = do(100)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_loop_train = news_loop[train_cases,]
  news_loop_test = news_loop[test_cases,]
  
lm1 = lm(shares ~ . , data=news_loop_train)
lm2 = lm(shares ~ . - n_tokens_content - self_reference_max_shares - max_positive_polarity -
           min_negative_polarity - weekday_is_sunday - is_weekend, data = news_loop_train)
lm3 = lm(shares ~ (. - n_tokens_content - self_reference_max_shares - max_positive_polarity -
           min_negative_polarity - weekday_is_sunday - is_weekend)^2, data = news_loop_train)
lm4 = lm(shares ~ n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + num_videos +
           average_token_length + num_keywords + data_channel_is_lifestyle + 
           data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + 
           data_channel_is_tech + data_channel_is_world + self_reference_min_shares + 
           self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + 
           weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + 
           weekday_is_saturday + global_rate_positive_words + global_rate_negative_words + 
           avg_positive_polarity + min_positive_polarity + avg_negative_polarity + 
           max_negative_polarity + title_subjectivity + title_sentiment_polarity + 
           abs_title_sentiment_polarity + self_reference_min_shares:self_reference_avg_sharess
         + num_self_hrefs:num_imgs + num_hrefs:data_channel_is_bus + 
           data_channel_is_socmed:avg_positive_polarity +
           num_keywords:global_rate_negative_words + n_tokens_title:num_self_hrefs + 
           num_self_hrefs:global_rate_positive_words + num_imgs:data_channel_is_world +
           data_channel_is_entertainment:min_positive_polarity + 
           num_hrefs:data_channel_is_tech, data=news_loop_train)
  yhat_test1 = predict(lm1, news_loop_test)
  yhat_test2 = predict(lm2, news_loop_test)
  yhat_test3 = predict(lm3, news_loop_test)
  yhat_test4 = predict(lm4, news_loop_test)
 
  c(error_rate(news_loop_test$shares, yhat_test1), error_rate(news_loop_test$shares, yhat_test2),
    error_rate(news_loop_test$shares, yhat_test3), error_rate(news_loop_test$shares, yhat_test4)) %>% round(3)
  
}

error_result= data.frame("lm1"= colMeans(err_vals[1]), "lm2"=colMeans(err_vals[2]), "lm3"=colMeans(err_vals[3]), "lm4"=colMeans(err_vals[4]), row.names="AVG Error Rate")

kable(error_result) %>% kable_styling("striped")
```

The table below reports the confusion matrix yielded by lm3 when using this model on the entire dataset. The model yielded an accuracy rate of 61.50% ((8051+16332)/39644) and an error rate of 38.50% ((12031+3230)/39644). Meanwhile, the true positive rate was 83.49% (16332/(3230+16332)), and the false positive rate was 59.91% (12031/(8051+12031)).

```{r 2.3.7, echo=FALSE, warning = FALSE}
news_articles = subset(dataset, select = -c(url))
news_articles$shares = log(news_articles$shares)
model_chosen = lm(shares ~ (. - n_tokens_content - self_reference_max_shares - 
                              max_positive_polarity - min_negative_polarity - weekday_is_sunday
                            - is_weekend)^2, data = news_articles)
news_articles$yhat = predict(model_chosen, news_articles)
news_articles = news_articles %>%
  mutate(viral = ifelse(shares>log(1400),1,0)) %>%
  mutate(viral_hat = ifelse(yhat>log(1400),1,0))
xtabs(~viral + viral_hat, data=news_articles)
```

The model is clearly predicting too many false positives. However, to assess its performance we need to compare this results to a "null" model, which would be to always predict the outcome that is most common in the data. Since 20082 out of 39644 articles didn't go viral, if we predicted all the articles not to go viral our accuracy rate would be of 50.66% (20082/39644). Hence, our model improved the accuracy rate from 50.66% to 61.50%. 

Another approach would be to  analyze this question from a classification perspective. We created a dummy variable viral, defined as as having over 1,400 shares or not. By having the response variable as a binomial, we were able to utilize the logit models which are listed below.

In the first logit model we included as explanatory variables all the available features. In the second, we excluded those that showed multicollinearity or too high p-values. In the third model, we used the explanatory variables that were included the linear model that yielded the highest accuracy rate. 

```{r 2.3.8, echo=TRUE}
glm1 = viral ~ n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs + num_imgs +               num_videos + average_token_length + num_keywords + data_channel_is_lifestyle +                 data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_max_shares + self_reference_avg_sharess + weekday_is_monday +                   weekday_is_tuesday + weekday_is_wednesday + weekday_is_thursday +                              weekday_is_friday + weekday_is_saturday + weekday_is_sunday + is_weekend +                     global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +               min_positive_polarity + max_positive_polarity + avg_negative_polarity +                        min_negative_polarity + max_negative_polarity + title_subjectivity +                           title_sentiment_polarity + abs_title_sentiment_polarity

glm2 = viral ~ n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + num_videos +                   average_token_length + num_keywords + data_channel_is_lifestyle +                              data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday +                          weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday +         global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +              min_positive_polarity + avg_negative_polarity + max_negative_polarity +                        title_subjectivity + title_sentiment_polarity + abs_title_sentiment_polarity

glm3 = viral ~ (n_tokens_title + num_hrefs + num_self_hrefs + num_imgs + num_videos +                  average_token_length + num_keywords + data_channel_is_lifestyle +                              data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed +                 data_channel_is_tech + data_channel_is_world + self_reference_min_shares +                     self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday +                          weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday +         global_rate_positive_words + global_rate_negative_words + avg_positive_polarity +              min_positive_polarity + avg_negative_polarity + max_negative_polarity +                        title_subjectivity + title_sentiment_polarity + abs_title_sentiment_polarity)^2
```

As with we did with the linear models, we used a bootstrap to randomly split the dataset into training and testing datasets containing 80% of observations and 20% of observations respectively. This process was repeated 50 times. For each repetition, we estimated three logit models using the train dataset and used each model's prediction on the test set to compare the accuracy rate when predicting which articles go viral.

The table shown below summarizes the average error rate for each of the three models. The results were averaged over 50 random train/test split samples in order to mollify the irregularities that could arise from randomization of samples. We can infer from the results that, on average, the third linear model yields the most accurate predictions.

```{r 2.3.9, echo=FALSE, warning=FALSE}
glm_loop = subset(dataset, select = -c(url))
glm_loop$viral = ifelse(glm_loop$shares > 1400, 1, 0)

prob_viral = sum(glm_loop$viral)/nrow(glm_loop)

n = nrow(glm_loop)
n_train = round(0.8*n)
n_test = n - n_train

err_vals_glm = do(50)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  glm_loop_train = glm_loop[train_cases,]
  glm_loop_test = glm_loop[test_cases,]
  
glm1 = glm(viral ~ . - shares, data=glm_loop_train, family = 'binomial')
glm2 = glm(viral ~ . - n_tokens_content - self_reference_max_shares - max_positive_polarity -
           min_negative_polarity - weekday_is_sunday - is_weekend - shares,
           data = glm_loop_train, family = 'binomial')
glm3 = glm(viral ~ (. - n_tokens_content - self_reference_max_shares - max_positive_polarity -
           min_negative_polarity - weekday_is_sunday - is_weekend - shares)^2,
           data = glm_loop_train, family = 'binomial')
  
glm_loop_test$yhat1 = predict(glm1, glm_loop_test, type='response')
glm_loop_test$yhat2 = predict(glm2, glm_loop_test, type='response')
glm_loop_test$yhat3 = predict(glm3, glm_loop_test, type='response')

glm_loop_test = glm_loop_test %>%
  mutate(viral_hat1 = ifelse(yhat1 >= prob_viral,1,0)) %>%
  mutate(viral_hat2 = ifelse(yhat2 >= prob_viral,1,0)) %>%
  mutate(viral_hat3 = ifelse(yhat3 >= prob_viral,1,0))
  
c(sum(glm_loop_test$viral != glm_loop_test$viral_hat1)/nrow(glm_loop_test),
  sum(glm_loop_test$viral != glm_loop_test$viral_hat2)/nrow(glm_loop_test),
  sum(glm_loop_test$viral != glm_loop_test$viral_hat3)/nrow(glm_loop_test)) %>% round(3)
}

error_result_glm = data.frame("glm1"= colMeans(err_vals_glm[1]), "glm2"=colMeans(err_vals_glm[2]), "glm3"=colMeans(err_vals_glm[3]), row.names="AVG Error Rate")

kable(error_result_glm) %>% kable_styling("striped")
```

The table below reports the confusion matrix yielded by glm3 when using this model on the entire dataset. The model yielded an accuracy rate of 65.02% ((12963+12813)/39644) and an error rate of 34.98% ((7119+6749)/39644). Meanwhile, the true positive rate was 65.5% (12813/(6749+12813)), and the false positive rate was 35.45% (7119/(12963+7119)).

```{r 2.3.10, echo=FALSE, warning = FALSE}
glm_chosen = glm(viral ~ (. - n_tokens_content - self_reference_max_shares -
                            max_positive_polarity - min_negative_polarity - weekday_is_sunday -
                            is_weekend - shares)^2, data = glm_loop)
glm_loop$yhat = predict(glm_chosen, glm_loop, type='response')
glm_loop = glm_loop %>%
    mutate(viral_hat = ifelse(yhat>=prob_viral,1,0))
xtabs(~viral + viral_hat, data=glm_loop)
```

Having assessed the accuracy rates of the best models from the first and the second the approach, the logit model turned out to perform slightly better, gaving a higher in-sample accuracy rate when trying to predict which article went viral.

```{r 2.3.11, echo=FALSE, warning = FALSE}

acc_summ = data.frame("lm"= sum(news_articles$viral == news_articles$viral_hat) / nrow(news_articles),"glm"= sum(glm_loop$viral == glm_loop$viral_hat)/nrow(glm_loop), row.names="Accuracy Rate")
                     
acc_summ = as.data.frame(t(acc_summ))

kable(acc_summ) %>% kable_styling("striped")

```

Finally, we used the K-nearest neighbors methodology to build a nonparametric predictive model and compare its results to those we found using linear and logistic models. As we did with the other models, we used a bootstrap to randomly split the dataset into training and testing datasets containing 80% of observations and 20% of observations respectively. This process was repeated 5 times. For each repetition, we estimated 15 possible Ks (from 5 to 75, by 5)and compared its predictions accuracy in the test dataset.

```{r 2.3.12, echo=FALSE, warning = FALSE}
online_news = dataset
online_news = mutate(online_news, viral = ifelse(online_news$shares > 1400, 1, 0))

X_all = model.matrix(~n_tokens_title + n_tokens_content + num_hrefs + num_self_hrefs +
                       num_imgs + num_videos + average_token_length + num_keywords +
                       data_channel_is_lifestyle + data_channel_is_entertainment +
                       data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech +
                       data_channel_is_world + self_reference_min_shares + self_reference_max_shares +
                       self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday +
                       weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday +
                       weekday_is_sunday + is_weekend + global_rate_positive_words + global_rate_negative_words +
                       avg_positive_polarity + min_positive_polarity + max_positive_polarity + avg_negative_polarity +
                       min_negative_polarity + max_negative_polarity + title_subjectivity + title_sentiment_polarity +
                       abs_title_sentiment_polarity - 1, data=online_news)

feature_sd = apply(X_all, 2, sd)
X_std = scale(X_all, scale=feature_sd)
k_grid = seq(10, 75, by=5)

n = nrow(X_all)
n_train = round(0.8*n)  
n_test = n - n_train

# loop over the individual data points for leave-one-out
boot1 = do(5)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  
  X_train = X_std[train_cases,]
  X_test = X_std[test_cases,]
  
  y_train = online_news$viral[train_cases]
  y_test = online_news$viral[test_cases]
  
  # fit the models: loop over k
  knn_reg = foreach(k = k_grid, .combine='c') %do% {
    knn_fit = knn.reg(X_train, X_test, y_train, k)
    knn_testpred = ifelse(knn_fit$pred >= prob_viral, 1, 0)
    conf_matrix = table(y = y_test, yhat= knn_testpred)
    accuracy = sum(diag(conf_matrix))/sum(conf_matrix)
    accuracy
  }
  
  # return results from the loop over k
  knn_reg
}

knn_summ= data.frame("1"= colMeans(boot1[1]), "2"= colMeans(boot1[2]), "3"= colMeans(boot1[3]),
                     "4"= colMeans(boot1[4]), "5"= colMeans(boot1[5]), "6"= colMeans(boot1[6]),
                     "7"= colMeans(boot1[7]), "8"= colMeans(boot1[8]), "9"= colMeans(boot1[9]),
              "10"= colMeans(boot1[10]), "11"= colMeans(boot1[11]), "12"= colMeans(boot1[12]),
                     "13"= colMeans(boot1[13]), "14"= colMeans(boot1[14]),
              row.names="Accuracy Rate")
                     
knn_summ = as.data.frame(t(knn_summ))
knn_summ$K = (1:nrow(knn_summ))*5

k_max = with(knn_summ, K[`Accuracy Rate` == max(`Accuracy Rate`)])
acc_max = with(knn_summ, `Accuracy Rate`[`Accuracy Rate` == max(`Accuracy Rate`)])

ggplot(data = knn_summ) + 
  theme_bw(base_size=18) +
  geom_path(aes(x = K, y = `Accuracy Rate`), colour = 'red') +
  scale_color_discrete(name = "Models") +
  geom_vline(xintercept = k_max, color = "black")
```

We found that the optimal K and the average accuracy yielded by it was the following.

```{r 2.3.13, echo=FALSE}
k_tab = data.frame("K"= k_max,"Accuracy" = acc_max)
kable(k_tab) %>% kable_styling("striped")
```

Finally, we can conclude that the logit model (lm3) performed better than the other models we tested. This model yielded a 63.79% out of sample accuracy rate, and a 65.02% in sample accuracy rate. When we compare it to the null-model (50.66%), improvement in the out of sample accuracy rate reaches 28.35% (65.02/50.66 - 1).

As a conjecture, our best judgement is that the difference in the performance between each approach possibly occured due to the fact that, when trying to predict the number of shares, the discrepancy in the frequency distrbution observed in the first histogram ends up affecting the variance of the estimates. The logistic model we used don't have that problem since its outcome is a probability (so bounded between 0 and 1) and the fitted values that overcame the in-sample probability of being viral as the models predictions.